{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2E2uKD2vS06kbOBzOxPyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ezzaldin97/pytorch-coding/blob/main/LinearRegression_in_numpy_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pCuwjJReyckB",
        "outputId": "d10f2832-3a45-49cb-c1f2-d61ff07d7936"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from numpy.random import random\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = random((30,2))\n",
        "weights = np.array([[2, -3]], dtype = np.float16)\n",
        "b = np.array([1], dtype = np.float16)"
      ],
      "metadata": {
        "id": "49lTDuZVzCu_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q15Wj47cTsvP",
        "outputId": "c7abedc7-2f26-4cc2-f8c9-a80ba71c7119"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.dot(x, [2., -3.]) + 1\n",
        "y = y.reshape(-1, 1)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tkO9gg10VZC",
        "outputId": "917433e9-b14e-4c4d-9941-33864287e6a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.93976215],\n",
              "       [ 0.02434462],\n",
              "       [ 0.08039168],\n",
              "       [ 1.5002215 ],\n",
              "       [ 1.14302064],\n",
              "       [ 0.11809923],\n",
              "       [ 2.58006522],\n",
              "       [ 0.92832671],\n",
              "       [ 2.01561069],\n",
              "       [ 0.12534904],\n",
              "       [ 2.08160314],\n",
              "       [-0.93065577],\n",
              "       [-0.65141736],\n",
              "       [ 1.22488698],\n",
              "       [-0.15569628],\n",
              "       [ 1.41021714],\n",
              "       [ 2.00527227],\n",
              "       [-0.2087599 ],\n",
              "       [ 0.60392554],\n",
              "       [ 1.07941942],\n",
              "       [-0.28899717],\n",
              "       [ 0.17633531],\n",
              "       [-1.51756306],\n",
              "       [-0.77860259],\n",
              "       [ 1.29081597],\n",
              "       [ 1.07168173],\n",
              "       [ 1.42321744],\n",
              "       [ 0.78026244],\n",
              "       [-0.13846486],\n",
              "       [-1.2772521 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define learnable weights.\n",
        "# random intialization for W and b\n",
        "\n",
        "W = random(2).reshape(1, -1)\n",
        "b_init = random(1).reshape(-1, 1)\n",
        "\n",
        "print(f\"intialized weights: {W}, {b_init}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiWSB7oF07w5",
        "outputId": "39c6934d-d88d-41a0-b213-52dc7b3e3779"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intialized weights: [[0.93086268 0.98144221]], [[0.67178206]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(W, b, X):\n",
        "  return X @ W.T +b\n",
        "y_hat = forward(W, b, x)\n",
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouOnifmW23eV",
        "outputId": "4cf53668-6e2d-4e7c-bf2c-a98ac806d30a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.05712832],\n",
              "       [1.32365771],\n",
              "       [2.15151457],\n",
              "       [2.25799384],\n",
              "       [1.08071281],\n",
              "       [1.6399012 ],\n",
              "       [1.93880159],\n",
              "       [1.9245289 ],\n",
              "       [1.75214286],\n",
              "       [2.26519005],\n",
              "       [1.61546524],\n",
              "       [1.98834992],\n",
              "       [2.1451757 ],\n",
              "       [1.51387152],\n",
              "       [1.58728088],\n",
              "       [1.46857939],\n",
              "       [1.65455939],\n",
              "       [2.4028588 ],\n",
              "       [2.31964251],\n",
              "       [1.55851195],\n",
              "       [1.61733357],\n",
              "       [2.63071005],\n",
              "       [1.89144548],\n",
              "       [1.85504375],\n",
              "       [1.34634378],\n",
              "       [1.79748713],\n",
              "       [1.32228793],\n",
              "       [2.5484004 ],\n",
              "       [1.51626786],\n",
              "       [2.14487546]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_hat, y):\n",
        "  return np.sum((y_hat - y)**2)\n",
        "loss(y_hat, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mupTEmq3_pj",
        "outputId": "f60d03ac-9201-48d4-dde3-16cd7bc1ddec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.3036494268948"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dl/dW, dl/db_init\n",
        "def grad(y_hat, y, X):\n",
        "  dW = (y_hat - y).T @ X\n",
        "  db = np.sum((y_hat - y))\n",
        "  return 2*dW, 2*db"
      ],
      "metadata": {
        "id": "PuSNFRyk4qn8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad(y_hat, y, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLPnA5KjbgOj",
        "outputId": "5b24407d-11db-4b31-b280-997f2d5a0837"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[29.29514833, 54.23357262]]), 77.32128561189221)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  y_hat = forward(W, b_init, x)\n",
        "  l = loss(y_hat, y)\n",
        "  dW, db = grad(y_hat, y, x)\n",
        "  # gradient descent step\n",
        "  W = W - lr * dW\n",
        "  b_init = b_init - lr * db\n",
        "  print(\"progress:\", \"epoch:\", epoch, \"loss\", l)\n",
        "# After training\n",
        "print(\"estimation of the parameters:\", W, b_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0mQOU_A619M",
        "outputId": "9419cf8d-8103-409b-bf2f-71a8f94874d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 70.15722711561119\n",
            "progress: epoch: 1 loss 64.57054261248092\n",
            "progress: epoch: 2 loss 59.83671189873473\n",
            "progress: epoch: 3 loss 55.81776820142183\n",
            "progress: epoch: 4 loss 52.39814151219098\n",
            "progress: epoch: 5 loss 49.48102208163298\n",
            "progress: epoch: 6 loss 46.985314370803856\n",
            "progress: epoch: 7 loss 44.84308558769412\n",
            "progress: epoch: 8 loss 42.997428503135716\n",
            "progress: epoch: 9 loss 41.400671279804286\n",
            "progress: epoch: 10 loss 40.01287796997699\n",
            "progress: epoch: 11 loss 38.800592486308105\n",
            "progress: epoch: 12 loss 37.7357865130336\n",
            "progress: epoch: 13 loss 36.79497824390237\n",
            "progress: epoch: 14 loss 35.95849420978717\n",
            "progress: epoch: 15 loss 35.20985096257901\n",
            "progress: epoch: 16 loss 34.5352371543635\n",
            "progress: epoch: 17 loss 33.9230797107509\n",
            "progress: epoch: 18 loss 33.36368044403758\n",
            "progress: epoch: 19 loss 32.84891166892231\n",
            "progress: epoch: 20 loss 32.37196124056535\n",
            "progress: epoch: 21 loss 31.92711899031282\n",
            "progress: epoch: 22 loss 31.509597837371633\n",
            "progress: epoch: 23 loss 31.115383946121607\n",
            "progress: epoch: 24 loss 30.741111212942716\n",
            "progress: epoch: 25 loss 30.383956132189528\n",
            "progress: epoch: 26 loss 30.041549732364548\n",
            "progress: epoch: 27 loss 29.711903810813926\n",
            "progress: epoch: 28 loss 29.39334914530516\n",
            "progress: epoch: 29 loss 29.084483737809634\n",
            "progress: epoch: 30 loss 28.78412946156936\n",
            "progress: epoch: 31 loss 28.491295747014032\n",
            "progress: epoch: 32 loss 28.205149163637135\n",
            "progress: epoch: 33 loss 27.924987940510327\n",
            "progress: epoch: 34 loss 27.650220623555164\n",
            "progress: epoch: 35 loss 27.380348197891987\n",
            "progress: epoch: 36 loss 27.114949112646464\n",
            "progress: epoch: 37 loss 26.853666736946078\n",
            "progress: epoch: 38 loss 26.596198852358746\n",
            "progress: epoch: 39 loss 26.3422888511204\n",
            "progress: epoch: 40 loss 26.091718363186583\n",
            "progress: epoch: 41 loss 25.844301080113482\n",
            "progress: epoch: 42 loss 25.599877581442943\n",
            "progress: epoch: 43 loss 25.35831100081814\n",
            "progress: epoch: 44 loss 25.11948339548643\n",
            "progress: epoch: 45 loss 24.88329270498339\n",
            "progress: epoch: 46 loss 24.64965020333607\n",
            "progress: epoch: 47 loss 24.41847836465565\n",
            "progress: epoch: 48 loss 24.189709075000675\n",
            "progress: epoch: 49 loss 23.963282134289702\n",
            "progress: epoch: 50 loss 23.739144001170974\n",
            "progress: epoch: 51 loss 23.517246741403177\n",
            "progress: epoch: 52 loss 23.297547146705803\n",
            "progress: epoch: 53 loss 23.080005996402868\n",
            "progress: epoch: 54 loss 22.864587438677276\n",
            "progress: epoch: 55 loss 22.65125847201739\n",
            "progress: epoch: 56 loss 22.439988510590283\n",
            "progress: epoch: 57 loss 22.23074901991696\n",
            "progress: epoch: 58 loss 22.023513211437447\n",
            "progress: epoch: 59 loss 21.81825578640614\n",
            "progress: epoch: 60 loss 21.61495272111036\n",
            "progress: epoch: 61 loss 21.413581086704806\n",
            "progress: epoch: 62 loss 21.214118898043978\n",
            "progress: epoch: 63 loss 21.01654498680649\n",
            "progress: epoch: 64 loss 20.820838894969455\n",
            "progress: epoch: 65 loss 20.62698078533112\n",
            "progress: epoch: 66 loss 20.43495136631599\n",
            "progress: epoch: 67 loss 20.24473182874577\n",
            "progress: epoch: 68 loss 20.056303792635482\n",
            "progress: epoch: 69 loss 19.869649262389377\n",
            "progress: epoch: 70 loss 19.684750589035012\n",
            "progress: epoch: 71 loss 19.501590438354896\n",
            "progress: epoch: 72 loss 19.3201517639605\n",
            "progress: epoch: 73 loss 19.140417784508177\n",
            "progress: epoch: 74 loss 18.962371964386833\n",
            "progress: epoch: 75 loss 18.78599799731568\n",
            "progress: epoch: 76 loss 18.61127979238182\n",
            "progress: epoch: 77 loss 18.438201462123534\n",
            "progress: epoch: 78 loss 18.26674731232932\n",
            "progress: epoch: 79 loss 18.096901833276103\n",
            "progress: epoch: 80 loss 17.928649692175075\n",
            "progress: epoch: 81 loss 17.761975726631107\n",
            "progress: epoch: 82 loss 17.59686493895315\n",
            "progress: epoch: 83 loss 17.433302491179525\n",
            "progress: epoch: 84 loss 17.271273700703993\n",
            "progress: epoch: 85 loss 17.110764036407016\n",
            "progress: epoch: 86 loss 16.951759115212138\n",
            "progress: epoch: 87 loss 16.794244699000423\n",
            "progress: epoch: 88 loss 16.638206691826674\n",
            "progress: epoch: 89 loss 16.483631137390418\n",
            "progress: epoch: 90 loss 16.330504216722044\n",
            "progress: epoch: 91 loss 16.17881224605116\n",
            "progress: epoch: 92 loss 16.02854167482934\n",
            "progress: epoch: 93 loss 15.879679083884021\n",
            "progress: epoch: 94 loss 15.73221118368412\n",
            "progress: epoch: 95 loss 15.586124812700955\n",
            "progress: epoch: 96 loss 15.44140693585084\n",
            "progress: epoch: 97 loss 15.29804464300776\n",
            "progress: epoch: 98 loss 15.156025147576614\n",
            "progress: epoch: 99 loss 15.015335785118774\n",
            "progress: epoch: 100 loss 14.875964012023314\n",
            "progress: epoch: 101 loss 14.73789740421803\n",
            "progress: epoch: 102 loss 14.601123655915645\n",
            "progress: epoch: 103 loss 14.465630578390988\n",
            "progress: epoch: 104 loss 14.331406098785846\n",
            "progress: epoch: 105 loss 14.19843825893861\n",
            "progress: epoch: 106 loss 14.066715214236288\n",
            "progress: epoch: 107 loss 13.936225232486855\n",
            "progress: epoch: 108 loss 13.806956692810239\n",
            "progress: epoch: 109 loss 13.678898084546445\n",
            "progress: epoch: 110 loss 13.552038006179632\n",
            "progress: epoch: 111 loss 13.42636516427706\n",
            "progress: epoch: 112 loss 13.301868372442007\n",
            "progress: epoch: 113 loss 13.178536550279937\n",
            "progress: epoch: 114 loss 13.056358722377205\n",
            "progress: epoch: 115 loss 12.935324017291814\n",
            "progress: epoch: 116 loss 12.815421666555642\n",
            "progress: epoch: 117 loss 12.696641003687812\n",
            "progress: epoch: 118 loss 12.57897146321879\n",
            "progress: epoch: 119 loss 12.462402579724888\n",
            "progress: epoch: 120 loss 12.346923986872913\n",
            "progress: epoch: 121 loss 12.232525416474715\n",
            "progress: epoch: 122 loss 12.119196697551384\n",
            "progress: epoch: 123 loss 12.006927755406906\n",
            "progress: epoch: 124 loss 11.895708610711129\n",
            "progress: epoch: 125 loss 11.785529378591825\n",
            "progress: epoch: 126 loss 11.676380267735768\n",
            "progress: epoch: 127 loss 11.568251579498558\n",
            "progress: epoch: 128 loss 11.461133707023272\n",
            "progress: epoch: 129 loss 11.355017134367593\n",
            "progress: epoch: 130 loss 11.249892435639467\n",
            "progress: epoch: 131 loss 11.145750274141115\n",
            "progress: epoch: 132 loss 11.04258140152131\n",
            "progress: epoch: 133 loss 10.940376656935797\n",
            "progress: epoch: 134 loss 10.839126966215845\n",
            "progress: epoch: 135 loss 10.738823341044718\n",
            "progress: epoch: 136 loss 10.639456878142074\n",
            "progress: epoch: 137 loss 10.541018758456193\n",
            "progress: epoch: 138 loss 10.443500246363893\n",
            "progress: epoch: 139 loss 10.346892688878144\n",
            "progress: epoch: 140 loss 10.251187514863211\n",
            "progress: epoch: 141 loss 10.156376234257335\n",
            "progress: epoch: 142 loss 10.062450437302791\n",
            "progress: epoch: 143 loss 9.969401793783346\n",
            "progress: epoch: 144 loss 9.877222052268916\n",
            "progress: epoch: 145 loss 9.785903039367522\n",
            "progress: epoch: 146 loss 9.695436658984285\n",
            "progress: epoch: 147 loss 9.605814891587542\n",
            "progress: epoch: 148 loss 9.517029793481942\n",
            "progress: epoch: 149 loss 9.429073496088458\n",
            "progress: epoch: 150 loss 9.341938205231264\n",
            "progress: epoch: 151 loss 9.255616200431405\n",
            "progress: epoch: 152 loss 9.170099834207207\n",
            "progress: epoch: 153 loss 9.08538153138132\n",
            "progress: epoch: 154 loss 9.001453788394388\n",
            "progress: epoch: 155 loss 8.918309172625243\n",
            "progress: epoch: 156 loss 8.835940321717572\n",
            "progress: epoch: 157 loss 8.754339942912999\n",
            "progress: epoch: 158 loss 8.673500812390499\n",
            "progress: epoch: 159 loss 8.593415774612133\n",
            "progress: epoch: 160 loss 8.514077741674987\n",
            "progress: epoch: 161 loss 8.435479692669281\n",
            "progress: epoch: 162 loss 8.357614673042596\n",
            "progress: epoch: 163 loss 8.280475793970156\n",
            "progress: epoch: 164 loss 8.204056231731087\n",
            "progress: epoch: 165 loss 8.12834922709064\n",
            "progress: epoch: 166 loss 8.053348084688253\n",
            "progress: epoch: 167 loss 7.9790461724314845\n",
            "progress: epoch: 168 loss 7.90543692089568\n",
            "progress: epoch: 169 loss 7.832513822729377\n",
            "progress: epoch: 170 loss 7.760270432065333\n",
            "progress: epoch: 171 loss 7.688700363937203\n",
            "progress: epoch: 172 loss 7.617797293701732\n",
            "progress: epoch: 173 loss 7.54755495646647\n",
            "progress: epoch: 174 loss 7.477967146522919\n",
            "progress: epoch: 175 loss 7.409027716785078\n",
            "progress: epoch: 176 loss 7.340730578233326\n",
            "progress: epoch: 177 loss 7.273069699363602\n",
            "progress: epoch: 178 loss 7.206039105641843\n",
            "progress: epoch: 179 loss 7.1396328789635675\n",
            "progress: epoch: 180 loss 7.073845157118636\n",
            "progress: epoch: 181 loss 7.008670133261121\n",
            "progress: epoch: 182 loss 6.944102055384154\n",
            "progress: epoch: 183 loss 6.880135225799847\n",
            "progress: epoch: 184 loss 6.816764000624107\n",
            "progress: epoch: 185 loss 6.7539827892663755\n",
            "progress: epoch: 186 loss 6.691786053924216\n",
            "progress: epoch: 187 loss 6.63016830908271\n",
            "progress: epoch: 188 loss 6.569124121018623\n",
            "progress: epoch: 189 loss 6.50864810730927\n",
            "progress: epoch: 190 loss 6.44873493634607\n",
            "progress: epoch: 191 loss 6.389379326852717\n",
            "progress: epoch: 192 loss 6.330576047407945\n",
            "progress: epoch: 193 loss 6.272319915972816\n",
            "progress: epoch: 194 loss 6.214605799422517\n",
            "progress: epoch: 195 loss 6.157428613082614\n",
            "progress: epoch: 196 loss 6.100783320269702\n",
            "progress: epoch: 197 loss 6.044664931836433\n",
            "progress: epoch: 198 loss 5.98906850572087\n",
            "progress: epoch: 199 loss 5.933989146500137\n",
            "progress: epoch: 200 loss 5.879422004948282\n",
            "progress: epoch: 201 loss 5.825362277598391\n",
            "progress: epoch: 202 loss 5.7718052063088265\n",
            "progress: epoch: 203 loss 5.718746077833609\n",
            "progress: epoch: 204 loss 5.666180223396892\n",
            "progress: epoch: 205 loss 5.6141030182714555\n",
            "progress: epoch: 206 loss 5.562509881361236\n",
            "progress: epoch: 207 loss 5.511396274787817\n",
            "progress: epoch: 208 loss 5.46075770348085\n",
            "progress: epoch: 209 loss 5.410589714772356\n",
            "progress: epoch: 210 loss 5.360887897994915\n",
            "progress: epoch: 211 loss 5.311647884083652\n",
            "progress: epoch: 212 loss 5.262865345182018\n",
            "progress: epoch: 213 loss 5.214535994251306\n",
            "progress: epoch: 214 loss 5.166655584683905\n",
            "progress: epoch: 215 loss 5.119219909920199\n",
            "progress: epoch: 216 loss 5.072224803069133\n",
            "progress: epoch: 217 loss 5.025666136532378\n",
            "progress: epoch: 218 loss 4.979539821632062\n",
            "progress: epoch: 219 loss 4.933841808242052\n",
            "progress: epoch: 220 loss 4.8885680844227295\n",
            "progress: epoch: 221 loss 4.843714676059253\n",
            "progress: epoch: 222 loss 4.799277646503233\n",
            "progress: epoch: 223 loss 4.755253096217851\n",
            "progress: epoch: 224 loss 4.71163716242631\n",
            "progress: epoch: 225 loss 4.668426018763661\n",
            "progress: epoch: 226 loss 4.625615874931913\n",
            "progress: epoch: 227 loss 4.583202976358445\n",
            "progress: epoch: 228 loss 4.541183603857639\n",
            "progress: epoch: 229 loss 4.49955407329575\n",
            "progress: epoch: 230 loss 4.458310735258954\n",
            "progress: epoch: 231 loss 4.417449974724546\n",
            "progress: epoch: 232 loss 4.376968210735262\n",
            "progress: epoch: 233 loss 4.336861896076709\n",
            "progress: epoch: 234 loss 4.297127516957849\n",
            "progress: epoch: 235 loss 4.25776159269452\n",
            "progress: epoch: 236 loss 4.218760675395964\n",
            "progress: epoch: 237 loss 4.18012134965434\n",
            "progress: epoch: 238 loss 4.141840232237185\n",
            "progress: epoch: 239 loss 4.103913971782793\n",
            "progress: epoch: 240 loss 4.066339248498485\n",
            "progress: epoch: 241 loss 4.02911277386176\n",
            "progress: epoch: 242 loss 3.992231290324271\n",
            "progress: epoch: 243 loss 3.9556915710186242\n",
            "progress: epoch: 244 loss 3.9194904194679556\n",
            "progress: epoch: 245 loss 3.8836246692982694\n",
            "progress: epoch: 246 loss 3.848091183953514\n",
            "progress: epoch: 247 loss 3.8128868564133525\n",
            "progress: epoch: 248 loss 3.778008608913633\n",
            "progress: epoch: 249 loss 3.743453392669495\n",
            "progress: epoch: 250 loss 3.7092181876011168\n",
            "progress: epoch: 251 loss 3.675300002062068\n",
            "progress: epoch: 252 loss 3.6416958725702413\n",
            "progress: epoch: 253 loss 3.6084028635413286\n",
            "progress: epoch: 254 loss 3.5754180670248537\n",
            "progress: epoch: 255 loss 3.542738602442688\n",
            "progress: epoch: 256 loss 3.5103616163300573\n",
            "progress: epoch: 257 loss 3.4782842820790156\n",
            "progress: epoch: 258 loss 3.446503799684346\n",
            "progress: epoch: 259 loss 3.4150173954918848\n",
            "progress: epoch: 260 loss 3.383822321949216\n",
            "progress: epoch: 261 loss 3.352915857358757\n",
            "progress: epoch: 262 loss 3.3222953056331797\n",
            "progress: epoch: 263 loss 3.2919579960531444\n",
            "progress: epoch: 264 loss 3.261901283027349\n",
            "progress: epoch: 265 loss 3.2321225458548466\n",
            "progress: epoch: 266 loss 3.202619188489629\n",
            "progress: epoch: 267 loss 3.1733886393074338\n",
            "progress: epoch: 268 loss 3.1444283508747755\n",
            "progress: epoch: 269 loss 3.115735799720169\n",
            "progress: epoch: 270 loss 3.087308486107513\n",
            "progress: epoch: 271 loss 3.05914393381164\n",
            "progress: epoch: 272 loss 3.0312396898959943\n",
            "progress: epoch: 273 loss 3.003593324492408\n",
            "progress: epoch: 274 loss 2.976202430582986\n",
            "progress: epoch: 275 loss 2.9490646237840474\n",
            "progress: epoch: 276 loss 2.922177542132124\n",
            "progress: epoch: 277 loss 2.895538845871992\n",
            "progress: epoch: 278 loss 2.8691462172467066\n",
            "progress: epoch: 279 loss 2.8429973602896417\n",
            "progress: epoch: 280 loss 2.8170900006184953\n",
            "progress: epoch: 281 loss 2.7914218852312525\n",
            "progress: epoch: 282 loss 2.7659907823040837\n",
            "progress: epoch: 283 loss 2.740794480991165\n",
            "progress: epoch: 284 loss 2.7158307912263937\n",
            "progress: epoch: 285 loss 2.6910975435269866\n",
            "progress: epoch: 286 loss 2.6665925887989417\n",
            "progress: epoch: 287 loss 2.642313798144354\n",
            "progress: epoch: 288 loss 2.6182590626705435\n",
            "progress: epoch: 289 loss 2.59442629330101\n",
            "progress: epoch: 290 loss 2.5708134205881765\n",
            "progress: epoch: 291 loss 2.5474183945279028\n",
            "progress: epoch: 292 loss 2.5242391843757708\n",
            "progress: epoch: 293 loss 2.5012737784651025\n",
            "progress: epoch: 294 loss 2.4785201840267113\n",
            "progress: epoch: 295 loss 2.4559764270103632\n",
            "progress: epoch: 296 loss 2.433640551907921\n",
            "progress: epoch: 297 loss 2.411510621578188\n",
            "progress: epoch: 298 loss 2.389584717073393\n",
            "progress: epoch: 299 loss 2.367860937467324\n",
            "progress: epoch: 300 loss 2.346337399685111\n",
            "progress: epoch: 301 loss 2.3250122383345957\n",
            "progress: epoch: 302 loss 2.3038836055393217\n",
            "progress: epoch: 303 loss 2.2829496707730996\n",
            "progress: epoch: 304 loss 2.2622086206961485\n",
            "progress: epoch: 305 loss 2.241658658992777\n",
            "progress: epoch: 306 loss 2.22129800621062\n",
            "progress: epoch: 307 loss 2.2011248996013943\n",
            "progress: epoch: 308 loss 2.1811375929631587\n",
            "progress: epoch: 309 loss 2.1613343564840815\n",
            "progress: epoch: 310 loss 2.1417134765876753\n",
            "progress: epoch: 311 loss 2.1222732557795143\n",
            "progress: epoch: 312 loss 2.103012012495395\n",
            "progress: epoch: 313 loss 2.0839280809509395\n",
            "progress: epoch: 314 loss 2.065019810992628\n",
            "progress: epoch: 315 loss 2.046285567950236\n",
            "progress: epoch: 316 loss 2.027723732490683\n",
            "progress: epoch: 317 loss 2.0093327004732524\n",
            "progress: epoch: 318 loss 1.991110882806198\n",
            "progress: epoch: 319 loss 1.973056705304697\n",
            "progress: epoch: 320 loss 1.955168608550158\n",
            "progress: epoch: 321 loss 1.9374450477508658\n",
            "progress: epoch: 322 loss 1.9198844926039333\n",
            "progress: epoch: 323 loss 1.9024854271585812\n",
            "progress: epoch: 324 loss 1.8852463496806948\n",
            "progress: epoch: 325 loss 1.868165772518676\n",
            "progress: epoch: 326 loss 1.8512422219705684\n",
            "progress: epoch: 327 loss 1.8344742381524313\n",
            "progress: epoch: 328 loss 1.8178603748679738\n",
            "progress: epoch: 329 loss 1.801399199479414\n",
            "progress: epoch: 330 loss 1.785089292779569\n",
            "progress: epoch: 331 loss 1.7689292488651525\n",
            "progress: epoch: 332 loss 1.7529176750112738\n",
            "progress: epoch: 333 loss 1.7370531915471366\n",
            "progress: epoch: 334 loss 1.7213344317328965\n",
            "progress: epoch: 335 loss 1.7057600416376988\n",
            "progress: epoch: 336 loss 1.6903286800188724\n",
            "progress: epoch: 337 loss 1.6750390182022619\n",
            "progress: epoch: 338 loss 1.6598897399636945\n",
            "progress: epoch: 339 loss 1.6448795414115784\n",
            "progress: epoch: 340 loss 1.630007130870599\n",
            "progress: epoch: 341 loss 1.6152712287665305\n",
            "progress: epoch: 342 loss 1.6006705675121355\n",
            "progress: epoch: 343 loss 1.586203891394133\n",
            "progress: epoch: 344 loss 1.5718699564612628\n",
            "progress: epoch: 345 loss 1.5576675304133833\n",
            "progress: epoch: 346 loss 1.543595392491634\n",
            "progress: epoch: 347 loss 1.5296523333696426\n",
            "progress: epoch: 348 loss 1.515837155045747\n",
            "progress: epoch: 349 loss 1.5021486707362517\n",
            "progress: epoch: 350 loss 1.488585704769692\n",
            "progress: epoch: 351 loss 1.475147092482095\n",
            "progress: epoch: 352 loss 1.4618316801132378\n",
            "progress: epoch: 353 loss 1.448638324703888\n",
            "progress: epoch: 354 loss 1.435565893994014\n",
            "progress: epoch: 355 loss 1.42261326632196\n",
            "progress: epoch: 356 loss 1.409779330524576\n",
            "progress: epoch: 357 loss 1.3970629858382981\n",
            "progress: epoch: 358 loss 1.3844631418011584\n",
            "progress: epoch: 359 loss 1.3719787181557268\n",
            "progress: epoch: 360 loss 1.359608644752969\n",
            "progress: epoch: 361 loss 1.3473518614570215\n",
            "progress: epoch: 362 loss 1.3352073180508572\n",
            "progress: epoch: 363 loss 1.3231739741428534\n",
            "progress: epoch: 364 loss 1.311250799074242\n",
            "progress: epoch: 365 loss 1.2994367718274336\n",
            "progress: epoch: 366 loss 1.2877308809352084\n",
            "progress: epoch: 367 loss 1.2761321243907715\n",
            "progress: epoch: 368 loss 1.2646395095586618\n",
            "progress: epoch: 369 loss 1.253252053086488\n",
            "progress: epoch: 370 loss 1.2419687808175215\n",
            "progress: epoch: 371 loss 1.2307887277041025\n",
            "progress: epoch: 372 loss 1.219710937721871\n",
            "progress: epoch: 373 loss 1.2087344637848074\n",
            "progress: epoch: 374 loss 1.1978583676610768\n",
            "progress: epoch: 375 loss 1.1870817198896744\n",
            "progress: epoch: 376 loss 1.176403599697852\n",
            "progress: epoch: 377 loss 1.1658230949193342\n",
            "progress: epoch: 378 loss 1.1553393019132991\n",
            "progress: epoch: 379 loss 1.1449513254841384\n",
            "progress: epoch: 380 loss 1.1346582788019588\n",
            "progress: epoch: 381 loss 1.1244592833238563\n",
            "progress: epoch: 382 loss 1.1143534687159165\n",
            "progress: epoch: 383 loss 1.1043399727759675\n",
            "progress: epoch: 384 loss 1.0944179413570547\n",
            "progress: epoch: 385 loss 1.0845865282916385\n",
            "progress: epoch: 386 loss 1.074844895316519\n",
            "progress: epoch: 387 loss 1.0651922119984591\n",
            "progress: epoch: 388 loss 1.0556276556605229\n",
            "progress: epoch: 389 loss 1.0461504113090954\n",
            "progress: epoch: 390 loss 1.0367596715616096\n",
            "progress: epoch: 391 loss 1.0274546365749444\n",
            "progress: epoch: 392 loss 1.0182345139745093\n",
            "progress: epoch: 393 loss 1.009098518783987\n",
            "progress: epoch: 394 loss 1.0000458733557556\n",
            "progress: epoch: 395 loss 0.9910758073019561\n",
            "progress: epoch: 396 loss 0.9821875574262207\n",
            "progress: epoch: 397 loss 0.973380367656042\n",
            "progress: epoch: 398 loss 0.9646534889757794\n",
            "progress: epoch: 399 loss 0.9560061793603061\n",
            "progress: epoch: 400 loss 0.9474377037092805\n",
            "progress: epoch: 401 loss 0.9389473337820309\n",
            "progress: epoch: 402 loss 0.9305343481330711\n",
            "progress: epoch: 403 loss 0.9221980320482138\n",
            "progress: epoch: 404 loss 0.9139376774812884\n",
            "progress: epoch: 405 loss 0.9057525829914682\n",
            "progress: epoch: 406 loss 0.8976420536811729\n",
            "progress: epoch: 407 loss 0.8896054011345729\n",
            "progress: epoch: 408 loss 0.8816419433566673\n",
            "progress: epoch: 409 loss 0.8737510047129387\n",
            "progress: epoch: 410 loss 0.8659319158695815\n",
            "progress: epoch: 411 loss 0.85818401373429\n",
            "progress: epoch: 412 loss 0.850506641397609\n",
            "progress: epoch: 413 loss 0.8428991480748395\n",
            "progress: epoch: 414 loss 0.8353608890484918\n",
            "progress: epoch: 415 loss 0.8278912256112837\n",
            "progress: epoch: 416 loss 0.8204895250096732\n",
            "progress: epoch: 417 loss 0.8131551603879322\n",
            "progress: epoch: 418 loss 0.8058875107327405\n",
            "progress: epoch: 419 loss 0.7986859608183074\n",
            "progress: epoch: 420 loss 0.7915499011520166\n",
            "progress: epoch: 421 loss 0.7844787279205717\n",
            "progress: epoch: 422 loss 0.7774718429366635\n",
            "progress: epoch: 423 loss 0.7705286535861342\n",
            "progress: epoch: 424 loss 0.7636485727756412\n",
            "progress: epoch: 425 loss 0.7568310188808173\n",
            "progress: epoch: 426 loss 0.750075415694919\n",
            "progress: epoch: 427 loss 0.743381192377961\n",
            "progress: epoch: 428 loss 0.7367477834063295\n",
            "progress: epoch: 429 loss 0.7301746285228716\n",
            "progress: epoch: 430 loss 0.7236611726874586\n",
            "progress: epoch: 431 loss 0.7172068660280128\n",
            "progress: epoch: 432 loss 0.7108111637919975\n",
            "progress: epoch: 433 loss 0.7044735262983677\n",
            "progress: epoch: 434 loss 0.6981934188899701\n",
            "progress: epoch: 435 loss 0.6919703118863951\n",
            "progress: epoch: 436 loss 0.6858036805372747\n",
            "progress: epoch: 437 loss 0.6796930049760224\n",
            "progress: epoch: 438 loss 0.6736377701740012\n",
            "progress: epoch: 439 loss 0.6676374658951372\n",
            "progress: epoch: 440 loss 0.6616915866509532\n",
            "progress: epoch: 441 loss 0.6557996316560255\n",
            "progress: epoch: 442 loss 0.649961104783869\n",
            "progress: epoch: 443 loss 0.6441755145232311\n",
            "progress: epoch: 444 loss 0.638442373934803\n",
            "progress: epoch: 445 loss 0.632761200608334\n",
            "progress: epoch: 446 loss 0.6271315166201589\n",
            "progress: epoch: 447 loss 0.6215528484911172\n",
            "progress: epoch: 448 loss 0.6160247271448763\n",
            "progress: epoch: 449 loss 0.6105466878666435\n",
            "progress: epoch: 450 loss 0.6051182702622688\n",
            "progress: epoch: 451 loss 0.5997390182177378\n",
            "progress: epoch: 452 loss 0.5944084798590382\n",
            "progress: epoch: 453 loss 0.5891262075124127\n",
            "progress: epoch: 454 loss 0.5838917576649824\n",
            "progress: epoch: 455 loss 0.5787046909257444\n",
            "progress: epoch: 456 loss 0.5735645719869361\n",
            "progress: epoch: 457 loss 0.5684709695857624\n",
            "progress: epoch: 458 loss 0.563423456466488\n",
            "progress: epoch: 459 loss 0.5584216093428804\n",
            "progress: epoch: 460 loss 0.553465008861016\n",
            "progress: epoch: 461 loss 0.5485532395624315\n",
            "progress: epoch: 462 loss 0.5436858898476198\n",
            "progress: epoch: 463 loss 0.538862551939879\n",
            "progress: epoch: 464 loss 0.5340828218494945\n",
            "progress: epoch: 465 loss 0.5293462993382632\n",
            "progress: epoch: 466 loss 0.5246525878843462\n",
            "progress: epoch: 467 loss 0.5200012946474651\n",
            "progress: epoch: 468 loss 0.515392030434413\n",
            "progress: epoch: 469 loss 0.5108244096648954\n",
            "progress: epoch: 470 loss 0.5062980503376991\n",
            "progress: epoch: 471 loss 0.5018125739971709\n",
            "progress: epoch: 472 loss 0.4973676057000175\n",
            "progress: epoch: 473 loss 0.4929627739824182\n",
            "progress: epoch: 474 loss 0.48859771082744363\n",
            "progress: epoch: 475 loss 0.48427205163278814\n",
            "progress: epoch: 476 loss 0.47998543517879955\n",
            "progress: epoch: 477 loss 0.47573750359681555\n",
            "progress: epoch: 478 loss 0.4715279023377953\n",
            "progress: epoch: 479 loss 0.46735628014124636\n",
            "progress: epoch: 480 loss 0.4632222890044483\n",
            "progress: epoch: 481 loss 0.45912558415196075\n",
            "progress: epoch: 482 loss 0.45506582400542317\n",
            "progress: epoch: 483 loss 0.4510426701536375\n",
            "progress: epoch: 484 loss 0.4470557873229322\n",
            "progress: epoch: 485 loss 0.44310484334780675\n",
            "progress: epoch: 486 loss 0.43918950914184984\n",
            "progress: epoch: 487 loss 0.4353094586689382\n",
            "progress: epoch: 488 loss 0.4314643689146962\n",
            "progress: epoch: 489 loss 0.42765391985823553\n",
            "progress: epoch: 490 loss 0.42387779444415213\n",
            "progress: epoch: 491 loss 0.4201356785547916\n",
            "progress: epoch: 492 loss 0.4164272609827743\n",
            "progress: epoch: 493 loss 0.4127522334037824\n",
            "progress: epoch: 494 loss 0.4091102903495941\n",
            "progress: epoch: 495 loss 0.4055011291813848\n",
            "progress: epoch: 496 loss 0.40192445006326816\n",
            "progress: epoch: 497 loss 0.3983799559360916\n",
            "progress: epoch: 498 loss 0.3948673524914815\n",
            "progress: epoch: 499 loss 0.39138634814612416\n",
            "progress: epoch: 500 loss 0.3879366540162964\n",
            "progress: epoch: 501 loss 0.38451798389263225\n",
            "progress: epoch: 502 loss 0.3811300542151314\n",
            "progress: epoch: 503 loss 0.3777725840483933\n",
            "progress: epoch: 504 loss 0.3744452950570975\n",
            "progress: epoch: 505 loss 0.37114791148170523\n",
            "progress: epoch: 506 loss 0.367880160114391\n",
            "progress: epoch: 507 loss 0.36464177027520434\n",
            "progress: epoch: 508 loss 0.3614324737884527\n",
            "progress: epoch: 509 loss 0.35825200495930865\n",
            "progress: epoch: 510 loss 0.35510010055063274\n",
            "progress: epoch: 511 loss 0.3519764997600226\n",
            "progress: epoch: 512 loss 0.3488809441970697\n",
            "progress: epoch: 513 loss 0.3458131778608357\n",
            "progress: epoch: 514 loss 0.34277294711753553\n",
            "progress: epoch: 515 loss 0.33976000067843687\n",
            "progress: epoch: 516 loss 0.33677408957796146\n",
            "progress: epoch: 517 loss 0.333814967151995\n",
            "progress: epoch: 518 loss 0.3308823890164042\n",
            "progress: epoch: 519 loss 0.32797611304574825\n",
            "progress: epoch: 520 loss 0.3250958993521986\n",
            "progress: epoch: 521 loss 0.3222415102646503\n",
            "progress: epoch: 522 loss 0.31941271030803425\n",
            "progress: epoch: 523 loss 0.31660926618282276\n",
            "progress: epoch: 524 loss 0.31383094674472706\n",
            "progress: epoch: 525 loss 0.31107752298458724\n",
            "progress: epoch: 526 loss 0.30834876800844635\n",
            "progress: epoch: 527 loss 0.3056444570178212\n",
            "progress: epoch: 528 loss 0.30296436729014925\n",
            "progress: epoch: 529 loss 0.3003082781594247\n",
            "progress: epoch: 530 loss 0.2976759709970133\n",
            "progress: epoch: 531 loss 0.29506722919265127\n",
            "progress: epoch: 532 loss 0.2924818381356203\n",
            "progress: epoch: 533 loss 0.28991958519609795\n",
            "progress: epoch: 534 loss 0.2873802597066886\n",
            "progress: epoch: 535 loss 0.284863652944121\n",
            "progress: epoch: 536 loss 0.2823695581111243\n",
            "progress: epoch: 537 loss 0.27989777031847063\n",
            "progress: epoch: 538 loss 0.27744808656718545\n",
            "progress: epoch: 539 loss 0.2750203057309303\n",
            "progress: epoch: 540 loss 0.2726142285385429\n",
            "progress: epoch: 541 loss 0.2702296575567514\n",
            "progress: epoch: 542 loss 0.2678663971730412\n",
            "progress: epoch: 543 loss 0.2655242535786891\n",
            "progress: epoch: 544 loss 0.2632030347519542\n",
            "progress: epoch: 545 loss 0.2609025504414273\n",
            "progress: epoch: 546 loss 0.258622612149536\n",
            "progress: epoch: 547 loss 0.25636303311620756\n",
            "progress: epoch: 548 loss 0.25412362830267976\n",
            "progress: epoch: 549 loss 0.2519042143754695\n",
            "progress: epoch: 550 loss 0.2497046096904861\n",
            "progress: epoch: 551 loss 0.24752463427730148\n",
            "progress: epoch: 552 loss 0.24536410982356105\n",
            "progress: epoch: 553 loss 0.24322285965954188\n",
            "progress: epoch: 554 loss 0.24110070874286008\n",
            "progress: epoch: 555 loss 0.238997483643318\n",
            "progress: epoch: 556 loss 0.23691301252789418\n",
            "progress: epoch: 557 loss 0.23484712514587602\n",
            "progress: epoch: 558 loss 0.23279965281412904\n",
            "progress: epoch: 559 loss 0.23077042840250744\n",
            "progress: epoch: 560 loss 0.22875928631939857\n",
            "progress: epoch: 561 loss 0.22676606249740563\n",
            "progress: epoch: 562 loss 0.22479059437916354\n",
            "progress: epoch: 563 loss 0.22283272090328818\n",
            "progress: epoch: 564 loss 0.22089228249045662\n",
            "progress: epoch: 565 loss 0.21896912102961885\n",
            "progress: epoch: 566 loss 0.21706307986433893\n",
            "progress: epoch: 567 loss 0.2151740037792636\n",
            "progress: epoch: 568 loss 0.21330173898671867\n",
            "progress: epoch: 569 loss 0.21144613311342914\n",
            "progress: epoch: 570 loss 0.20960703518736734\n",
            "progress: epoch: 571 loss 0.2077842956247215\n",
            "progress: epoch: 572 loss 0.20597776621698663\n",
            "progress: epoch: 573 loss 0.20418730011817846\n",
            "progress: epoch: 574 loss 0.2024127518321649\n",
            "progress: epoch: 575 loss 0.20065397720011852\n",
            "progress: epoch: 576 loss 0.1989108333880848\n",
            "progress: epoch: 577 loss 0.1971831788746683\n",
            "progress: epoch: 578 loss 0.19547087343883293\n",
            "progress: epoch: 579 loss 0.19377377814781838\n",
            "progress: epoch: 580 loss 0.19209175534516762\n",
            "progress: epoch: 581 loss 0.19042466863886737\n",
            "progress: epoch: 582 loss 0.18877238288960246\n",
            "progress: epoch: 583 loss 0.18713476419911332\n",
            "progress: epoch: 584 loss 0.18551167989867448\n",
            "progress: epoch: 585 loss 0.18390299853766454\n",
            "progress: epoch: 586 loss 0.18230858987226012\n",
            "progress: epoch: 587 loss 0.18072832485422433\n",
            "progress: epoch: 588 loss 0.17916207561980338\n",
            "progress: epoch: 589 loss 0.17760971547872975\n",
            "progress: epoch: 590 loss 0.1760711189033251\n",
            "progress: epoch: 591 loss 0.1745461615177071\n",
            "progress: epoch: 592 loss 0.17303472008709608\n",
            "progress: epoch: 593 loss 0.17153667250722404\n",
            "progress: epoch: 594 loss 0.1700518977938399\n",
            "progress: epoch: 595 loss 0.1685802760723138\n",
            "progress: epoch: 596 loss 0.16712168856734194\n",
            "progress: epoch: 597 loss 0.165676017592743\n",
            "progress: epoch: 598 loss 0.1642431465413526\n",
            "progress: epoch: 599 loss 0.1628229598750138\n",
            "progress: epoch: 600 loss 0.16141534311465805\n",
            "progress: epoch: 601 loss 0.16002018283048103\n",
            "progress: epoch: 602 loss 0.15863736663221\n",
            "progress: epoch: 603 loss 0.15726678315946263\n",
            "progress: epoch: 604 loss 0.1559083220721949\n",
            "progress: epoch: 605 loss 0.15456187404123994\n",
            "progress: epoch: 606 loss 0.1532273307389339\n",
            "progress: epoch: 607 loss 0.15190458482983105\n",
            "progress: epoch: 608 loss 0.1505935299615046\n",
            "progress: epoch: 609 loss 0.14929406075543442\n",
            "progress: epoch: 610 loss 0.14800607279797806\n",
            "progress: epoch: 611 loss 0.14672946263142975\n",
            "progress: epoch: 612 loss 0.14546412774515974\n",
            "progress: epoch: 613 loss 0.14420996656683838\n",
            "progress: epoch: 614 loss 0.14296687845374165\n",
            "progress: epoch: 615 loss 0.14173476368413743\n",
            "progress: epoch: 616 loss 0.14051352344875392\n",
            "progress: epoch: 617 loss 0.13930305984232633\n",
            "progress: epoch: 618 loss 0.1381032758552231\n",
            "progress: epoch: 619 loss 0.13691407536515118\n",
            "progress: epoch: 620 loss 0.13573536312893653\n",
            "progress: epoch: 621 loss 0.1345670447743855\n",
            "progress: epoch: 622 loss 0.13340902679221825\n",
            "progress: epoch: 623 loss 0.13226121652807935\n",
            "progress: epoch: 624 loss 0.13112352217462306\n",
            "progress: epoch: 625 loss 0.12999585276367287\n",
            "progress: epoch: 626 loss 0.1288781181584536\n",
            "progress: epoch: 627 loss 0.12777022904589563\n",
            "progress: epoch: 628 loss 0.12667209692901304\n",
            "progress: epoch: 629 loss 0.12558363411935136\n",
            "progress: epoch: 630 loss 0.1245047537295061\n",
            "progress: epoch: 631 loss 0.12343536966571092\n",
            "progress: epoch: 632 loss 0.12237539662049571\n",
            "progress: epoch: 633 loss 0.1213247500654137\n",
            "progress: epoch: 634 loss 0.12028334624383434\n",
            "progress: epoch: 635 loss 0.11925110216380688\n",
            "progress: epoch: 636 loss 0.1182279355909866\n",
            "progress: epoch: 637 loss 0.11721376504163065\n",
            "progress: epoch: 638 loss 0.11620850977565704\n",
            "progress: epoch: 639 loss 0.11521208978976998\n",
            "progress: epoch: 640 loss 0.11422442581064768\n",
            "progress: epoch: 641 loss 0.1132454392881944\n",
            "progress: epoch: 642 loss 0.11227505238885838\n",
            "progress: epoch: 643 loss 0.11131318798900663\n",
            "progress: epoch: 644 loss 0.11035976966836561\n",
            "progress: epoch: 645 loss 0.10941472170352234\n",
            "progress: epoch: 646 loss 0.10847796906148537\n",
            "progress: epoch: 647 loss 0.10754943739330654\n",
            "progress: epoch: 648 loss 0.10662905302776206\n",
            "progress: epoch: 649 loss 0.10571674296509266\n",
            "progress: epoch: 650 loss 0.10481243487080137\n",
            "progress: epoch: 651 loss 0.10391605706951022\n",
            "progress: epoch: 652 loss 0.10302753853887484\n",
            "progress: epoch: 653 loss 0.10214680890355289\n",
            "progress: epoch: 654 loss 0.1012737984292322\n",
            "progress: epoch: 655 loss 0.1004084380167124\n",
            "progress: epoch: 656 loss 0.09955065919604235\n",
            "progress: epoch: 657 loss 0.09870039412071306\n",
            "progress: epoch: 658 loss 0.09785757556190336\n",
            "progress: epoch: 659 loss 0.09702213690278011\n",
            "progress: epoch: 660 loss 0.09619401213285156\n",
            "progress: epoch: 661 loss 0.09537313584237257\n",
            "progress: epoch: 662 loss 0.09455944321680315\n",
            "progress: epoch: 663 loss 0.0937528700313175\n",
            "progress: epoch: 664 loss 0.09295335264536586\n",
            "progress: epoch: 665 loss 0.0921608279972841\n",
            "progress: epoch: 666 loss 0.09137523359895754\n",
            "progress: epoch: 667 loss 0.0905965075305311\n",
            "progress: epoch: 668 loss 0.08982458843517002\n",
            "progress: epoch: 669 loss 0.08905941551386959\n",
            "progress: epoch: 670 loss 0.08830092852031353\n",
            "progress: epoch: 671 loss 0.087549067755779\n",
            "progress: epoch: 672 loss 0.08680377406409047\n",
            "progress: epoch: 673 loss 0.08606498882661955\n",
            "progress: epoch: 674 loss 0.08533265395733154\n",
            "progress: epoch: 675 loss 0.08460671189787966\n",
            "progress: epoch: 676 loss 0.08388710561274246\n",
            "progress: epoch: 677 loss 0.08317377858440801\n",
            "progress: epoch: 678 loss 0.08246667480860262\n",
            "progress: epoch: 679 loss 0.0817657387895645\n",
            "progress: epoch: 680 loss 0.08107091553536103\n",
            "progress: epoch: 681 loss 0.08038215055324838\n",
            "progress: epoch: 682 loss 0.07969938984507631\n",
            "progress: epoch: 683 loss 0.07902257990273517\n",
            "progress: epoch: 684 loss 0.07835166770364468\n",
            "progress: epoch: 685 loss 0.07768660070628454\n",
            "progress: epoch: 686 loss 0.0770273268457682\n",
            "progress: epoch: 687 loss 0.07637379452945554\n",
            "progress: epoch: 688 loss 0.07572595263260873\n",
            "progress: epoch: 689 loss 0.07508375049408662\n",
            "progress: epoch: 690 loss 0.07444713791207964\n",
            "progress: epoch: 691 loss 0.07381606513988544\n",
            "progress: epoch: 692 loss 0.07319048288172243\n",
            "progress: epoch: 693 loss 0.07257034228858307\n",
            "progress: epoch: 694 loss 0.0719555949541251\n",
            "progress: epoch: 695 loss 0.07134619291060205\n",
            "progress: epoch: 696 loss 0.07074208862483067\n",
            "progress: epoch: 697 loss 0.07014323499419572\n",
            "progress: epoch: 698 loss 0.06954958534269229\n",
            "progress: epoch: 699 loss 0.06896109341700542\n",
            "progress: epoch: 700 loss 0.0683777133826255\n",
            "progress: epoch: 701 loss 0.06779939981999926\n",
            "progress: epoch: 702 loss 0.06722610772071817\n",
            "progress: epoch: 703 loss 0.06665779248373979\n",
            "progress: epoch: 704 loss 0.06609440991164807\n",
            "progress: epoch: 705 loss 0.0655359162069434\n",
            "progress: epoch: 706 loss 0.06498226796837195\n",
            "progress: epoch: 707 loss 0.06443342218728518\n",
            "progress: epoch: 708 loss 0.06388933624403621\n",
            "progress: epoch: 709 loss 0.06334996790440763\n",
            "progress: epoch: 710 loss 0.06281527531607389\n",
            "progress: epoch: 711 loss 0.06228521700509477\n",
            "progress: epoch: 712 loss 0.061759751872443154\n",
            "progress: epoch: 713 loss 0.061238839190565386\n",
            "progress: epoch: 714 loss 0.060722438599970945\n",
            "progress: epoch: 715 loss 0.0602105101058561\n",
            "progress: epoch: 716 loss 0.059703014074758295\n",
            "progress: epoch: 717 loss 0.0591999112312418\n",
            "progress: epoch: 718 loss 0.058701162654613004\n",
            "progress: epoch: 719 loss 0.05820672977566803\n",
            "progress: epoch: 720 loss 0.05771657437346893\n",
            "progress: epoch: 721 loss 0.057230658572150366\n",
            "progress: epoch: 722 loss 0.05674894483775599\n",
            "progress: epoch: 723 loss 0.05627139597510476\n",
            "progress: epoch: 724 loss 0.0557979751246849\n",
            "progress: epoch: 725 loss 0.055328645759577855\n",
            "progress: epoch: 726 loss 0.0548633716824105\n",
            "progress: epoch: 727 loss 0.05440211702233484\n",
            "progress: epoch: 728 loss 0.05394484623203761\n",
            "progress: epoch: 729 loss 0.053491524084775185\n",
            "progress: epoch: 730 loss 0.053042115671437975\n",
            "progress: epoch: 731 loss 0.05259658639764086\n",
            "progress: epoch: 732 loss 0.05215490198084105\n",
            "progress: epoch: 733 loss 0.05171702844748294\n",
            "progress: epoch: 734 loss 0.05128293213016866\n",
            "progress: epoch: 735 loss 0.050852579664854694\n",
            "progress: epoch: 736 loss 0.05042593798807744\n",
            "progress: epoch: 737 loss 0.050002974334198654\n",
            "progress: epoch: 738 loss 0.04958365623268298\n",
            "progress: epoch: 739 loss 0.04916795150539588\n",
            "progress: epoch: 740 loss 0.04875582826392838\n",
            "progress: epoch: 741 loss 0.04834725490694726\n",
            "progress: epoch: 742 loss 0.04794220011756832\n",
            "progress: epoch: 743 loss 0.047540632860755064\n",
            "progress: epoch: 744 loss 0.04714252238074135\n",
            "progress: epoch: 745 loss 0.04674783819847709\n",
            "progress: epoch: 746 loss 0.046356550109099004\n",
            "progress: epoch: 747 loss 0.04596862817942271\n",
            "progress: epoch: 748 loss 0.04558404274546045\n",
            "progress: epoch: 749 loss 0.04520276440996003\n",
            "progress: epoch: 750 loss 0.04482476403996766\n",
            "progress: epoch: 751 loss 0.04445001276441214\n",
            "progress: epoch: 752 loss 0.04407848197171241\n",
            "progress: epoch: 753 loss 0.04371014330740755\n",
            "progress: epoch: 754 loss 0.043344968671806955\n",
            "progress: epoch: 755 loss 0.04298293021766401\n",
            "progress: epoch: 756 loss 0.04262400034787067\n",
            "progress: epoch: 757 loss 0.04226815171317307\n",
            "progress: epoch: 758 loss 0.04191535720990896\n",
            "progress: epoch: 759 loss 0.04156558997776547\n",
            "progress: epoch: 760 loss 0.04121882339755716\n",
            "progress: epoch: 761 loss 0.040875031089026126\n",
            "progress: epoch: 762 loss 0.04053418690866159\n",
            "progress: epoch: 763 loss 0.04019626494753898\n",
            "progress: epoch: 764 loss 0.0398612395291795\n",
            "progress: epoch: 765 loss 0.03952908520743058\n",
            "progress: epoch: 766 loss 0.03919977676436422\n",
            "progress: epoch: 767 loss 0.03887328920819586\n",
            "progress: epoch: 768 loss 0.038549597771221314\n",
            "progress: epoch: 769 loss 0.03822867790777444\n",
            "progress: epoch: 770 loss 0.037910505292202856\n",
            "progress: epoch: 771 loss 0.037595055816861414\n",
            "progress: epoch: 772 loss 0.03728230559012561\n",
            "progress: epoch: 773 loss 0.03697223093442324\n",
            "progress: epoch: 774 loss 0.036664808384282814\n",
            "progress: epoch: 775 loss 0.0363600146844022\n",
            "progress: epoch: 776 loss 0.03605782678773282\n",
            "progress: epoch: 777 loss 0.03575822185358333\n",
            "progress: epoch: 778 loss 0.03546117724573947\n",
            "progress: epoch: 779 loss 0.03516667053060251\n",
            "progress: epoch: 780 loss 0.03487467947534377\n",
            "progress: epoch: 781 loss 0.034585182046077155\n",
            "progress: epoch: 782 loss 0.034298156406047636\n",
            "progress: epoch: 783 loss 0.03401358091383689\n",
            "progress: epoch: 784 loss 0.03373143412158573\n",
            "progress: epoch: 785 loss 0.03345169477323243\n",
            "progress: epoch: 786 loss 0.03317434180276753\n",
            "progress: epoch: 787 loss 0.03289935433250538\n",
            "progress: epoch: 788 loss 0.032626711671369955\n",
            "progress: epoch: 789 loss 0.03235639331319843\n",
            "progress: epoch: 790 loss 0.032088378935058566\n",
            "progress: epoch: 791 loss 0.03182264839558434\n",
            "progress: epoch: 792 loss 0.03155918173332258\n",
            "progress: epoch: 793 loss 0.0312979591650992\n",
            "progress: epoch: 794 loss 0.03103896108439836\n",
            "progress: epoch: 795 loss 0.03078216805975678\n",
            "progress: epoch: 796 loss 0.030527560833172318\n",
            "progress: epoch: 797 loss 0.030275120318528803\n",
            "progress: epoch: 798 loss 0.03002482760003375\n",
            "progress: epoch: 799 loss 0.029776663930671576\n",
            "progress: epoch: 800 loss 0.029530610730670665\n",
            "progress: epoch: 801 loss 0.029286649585984193\n",
            "progress: epoch: 802 loss 0.029044762246784945\n",
            "progress: epoch: 803 loss 0.028804930625975462\n",
            "progress: epoch: 804 loss 0.02856713679770952\n",
            "progress: epoch: 805 loss 0.0283313629959295\n",
            "progress: epoch: 806 loss 0.028097591612915394\n",
            "progress: epoch: 807 loss 0.027865805197848304\n",
            "progress: epoch: 808 loss 0.027635986455387035\n",
            "progress: epoch: 809 loss 0.02740811824425716\n",
            "progress: epoch: 810 loss 0.0271821835758538\n",
            "progress: epoch: 811 loss 0.02695816561285686\n",
            "progress: epoch: 812 loss 0.026736047667858805\n",
            "progress: epoch: 813 loss 0.026515813202006\n",
            "progress: epoch: 814 loss 0.02629744582365117\n",
            "progress: epoch: 815 loss 0.02608092928701995\n",
            "progress: epoch: 816 loss 0.02586624749088733\n",
            "progress: epoch: 817 loss 0.02565338447726876\n",
            "progress: epoch: 818 loss 0.02544232443012218\n",
            "progress: epoch: 819 loss 0.025233051674061497\n",
            "progress: epoch: 820 loss 0.02502555067308265\n",
            "progress: epoch: 821 loss 0.024819806029301342\n",
            "progress: epoch: 822 loss 0.024615802481701483\n",
            "progress: epoch: 823 loss 0.024413524904896875\n",
            "progress: epoch: 824 loss 0.02421295830790217\n",
            "progress: epoch: 825 loss 0.024014087832917023\n",
            "progress: epoch: 826 loss 0.023816898754120082\n",
            "progress: epoch: 827 loss 0.023621376476474273\n",
            "progress: epoch: 828 loss 0.023427506534544267\n",
            "progress: epoch: 829 loss 0.023235274591323123\n",
            "progress: epoch: 830 loss 0.023044666437070364\n",
            "progress: epoch: 831 loss 0.022855667988161403\n",
            "progress: epoch: 832 loss 0.022668265285946575\n",
            "progress: epoch: 833 loss 0.022482444495620636\n",
            "progress: epoch: 834 loss 0.02229819190510434\n",
            "progress: epoch: 835 loss 0.02211549392393303\n",
            "progress: epoch: 836 loss 0.021934337082159076\n",
            "progress: epoch: 837 loss 0.021754708029261483\n",
            "progress: epoch: 838 loss 0.021576593533067605\n",
            "progress: epoch: 839 loss 0.021399980478683068\n",
            "progress: epoch: 840 loss 0.02122485586743245\n",
            "progress: epoch: 841 loss 0.021051206815810117\n",
            "progress: epoch: 842 loss 0.020879020554438986\n",
            "progress: epoch: 843 loss 0.020708284427040997\n",
            "progress: epoch: 844 loss 0.02053898588941524\n",
            "progress: epoch: 845 loss 0.02037111250842626\n",
            "progress: epoch: 846 loss 0.020204651961001702\n",
            "progress: epoch: 847 loss 0.02003959203313912\n",
            "progress: epoch: 848 loss 0.019875920618921215\n",
            "progress: epoch: 849 loss 0.01971362571954125\n",
            "progress: epoch: 850 loss 0.01955269544233603\n",
            "progress: epoch: 851 loss 0.019393117999829335\n",
            "progress: epoch: 852 loss 0.019234881708782604\n",
            "progress: epoch: 853 loss 0.01907797498925534\n",
            "progress: epoch: 854 loss 0.018922386363673428\n",
            "progress: epoch: 855 loss 0.01876810445590673\n",
            "progress: epoch: 856 loss 0.018615117990354316\n",
            "progress: epoch: 857 loss 0.01846341579103875\n",
            "progress: epoch: 858 loss 0.018312986780708213\n",
            "progress: epoch: 859 loss 0.01816381997994717\n",
            "progress: epoch: 860 loss 0.01801590450629485\n",
            "progress: epoch: 861 loss 0.01786922957337196\n",
            "progress: epoch: 862 loss 0.017723784490015736\n",
            "progress: epoch: 863 loss 0.017579558659422047\n",
            "progress: epoch: 864 loss 0.017436541578296272\n",
            "progress: epoch: 865 loss 0.017294722836011606\n",
            "progress: epoch: 866 loss 0.01715409211377461\n",
            "progress: epoch: 867 loss 0.01701463918379883\n",
            "progress: epoch: 868 loss 0.016876353908486495\n",
            "progress: epoch: 869 loss 0.01673922623961619\n",
            "progress: epoch: 870 loss 0.016603246217539776\n",
            "progress: epoch: 871 loss 0.0164684039703856\n",
            "progress: epoch: 872 loss 0.01633468971326862\n",
            "progress: epoch: 873 loss 0.016202093747508636\n",
            "progress: epoch: 874 loss 0.01607060645985576\n",
            "progress: epoch: 875 loss 0.01594021832172148\n",
            "progress: epoch: 876 loss 0.01581091988841895\n",
            "progress: epoch: 877 loss 0.015682701798408367\n",
            "progress: epoch: 878 loss 0.015555554772549874\n",
            "progress: epoch: 879 loss 0.015429469613363892\n",
            "progress: epoch: 880 loss 0.015304437204296986\n",
            "progress: epoch: 881 loss 0.015180448508995553\n",
            "progress: epoch: 882 loss 0.015057494570585736\n",
            "progress: epoch: 883 loss 0.014935566510959673\n",
            "progress: epoch: 884 loss 0.014814655530068289\n",
            "progress: epoch: 885 loss 0.014694752905221285\n",
            "progress: epoch: 886 loss 0.014575849990392487\n",
            "progress: epoch: 887 loss 0.01445793821553187\n",
            "progress: epoch: 888 loss 0.014341009085884624\n",
            "progress: epoch: 889 loss 0.014225054181315033\n",
            "progress: epoch: 890 loss 0.014110065155637567\n",
            "progress: epoch: 891 loss 0.013996033735954126\n",
            "progress: epoch: 892 loss 0.013882951721996554\n",
            "progress: epoch: 893 loss 0.013770810985475801\n",
            "progress: epoch: 894 loss 0.013659603469436538\n",
            "progress: epoch: 895 loss 0.013549321187618034\n",
            "progress: epoch: 896 loss 0.013439956223820855\n",
            "progress: epoch: 897 loss 0.01333150073127926\n",
            "progress: epoch: 898 loss 0.01322394693203904\n",
            "progress: epoch: 899 loss 0.013117287116341251\n",
            "progress: epoch: 900 loss 0.013011513642011752\n",
            "progress: epoch: 901 loss 0.012906618933856253\n",
            "progress: epoch: 902 loss 0.012802595483060073\n",
            "progress: epoch: 903 loss 0.012699435846594971\n",
            "progress: epoch: 904 loss 0.012597132646629618\n",
            "progress: epoch: 905 loss 0.012495678569946732\n",
            "progress: epoch: 906 loss 0.01239506636736463\n",
            "progress: epoch: 907 loss 0.01229528885316504\n",
            "progress: epoch: 908 loss 0.012196338904525112\n",
            "progress: epoch: 909 loss 0.012098209460954992\n",
            "progress: epoch: 910 loss 0.012000893523740754\n",
            "progress: epoch: 911 loss 0.011904384155392125\n",
            "progress: epoch: 912 loss 0.011808674479095095\n",
            "progress: epoch: 913 loss 0.011713757678170062\n",
            "progress: epoch: 914 loss 0.01161962699553437\n",
            "progress: epoch: 915 loss 0.011526275733169793\n",
            "progress: epoch: 916 loss 0.011433697251595267\n",
            "progress: epoch: 917 loss 0.011341884969343739\n",
            "progress: epoch: 918 loss 0.011250832362444622\n",
            "progress: epoch: 919 loss 0.01116053296390996\n",
            "progress: epoch: 920 loss 0.011070980363226486\n",
            "progress: epoch: 921 loss 0.01098216820585084\n",
            "progress: epoch: 922 loss 0.010894090192710683\n",
            "progress: epoch: 923 loss 0.01080674007970952\n",
            "progress: epoch: 924 loss 0.01072011167723652\n",
            "progress: epoch: 925 loss 0.010634198849680278\n",
            "progress: epoch: 926 loss 0.010548995514947607\n",
            "progress: epoch: 927 loss 0.010464495643986091\n",
            "progress: epoch: 928 loss 0.010380693260311467\n",
            "progress: epoch: 929 loss 0.01029758243953876\n",
            "progress: epoch: 930 loss 0.01021515730891825\n",
            "progress: epoch: 931 loss 0.010133412046875605\n",
            "progress: epoch: 932 loss 0.010052340882555365\n",
            "progress: epoch: 933 loss 0.009971938095369738\n",
            "progress: epoch: 934 loss 0.009892198014550557\n",
            "progress: epoch: 935 loss 0.00981311501870597\n",
            "progress: epoch: 936 loss 0.009734683535380853\n",
            "progress: epoch: 937 loss 0.009656898040620596\n",
            "progress: epoch: 938 loss 0.009579753058540282\n",
            "progress: epoch: 939 loss 0.009503243160896528\n",
            "progress: epoch: 940 loss 0.009427362966663574\n",
            "progress: epoch: 941 loss 0.009352107141613338\n",
            "progress: epoch: 942 loss 0.009277470397899336\n",
            "progress: epoch: 943 loss 0.00920344749364391\n",
            "progress: epoch: 944 loss 0.00913003323252996\n",
            "progress: epoch: 945 loss 0.00905722246339553\n",
            "progress: epoch: 946 loss 0.008985010079832734\n",
            "progress: epoch: 947 loss 0.008913391019789853\n",
            "progress: epoch: 948 loss 0.008842360265177401\n",
            "progress: epoch: 949 loss 0.008771912841477325\n",
            "progress: epoch: 950 loss 0.008702043817356512\n",
            "progress: epoch: 951 loss 0.008632748304282479\n",
            "progress: epoch: 952 loss 0.00856402145614418\n",
            "progress: epoch: 953 loss 0.008495858468874986\n",
            "progress: epoch: 954 loss 0.008428254580079462\n",
            "progress: epoch: 955 loss 0.008361205068663512\n",
            "progress: epoch: 956 loss 0.008294705254468425\n",
            "progress: epoch: 957 loss 0.008228750497907006\n",
            "progress: epoch: 958 loss 0.008163336199604248\n",
            "progress: epoch: 959 loss 0.008098457800040308\n",
            "progress: epoch: 960 loss 0.008034110779197468\n",
            "progress: epoch: 961 loss 0.007970290656209796\n",
            "progress: epoch: 962 loss 0.00790699298901581\n",
            "progress: epoch: 963 loss 0.007844213374015255\n",
            "progress: epoch: 964 loss 0.0077819474457275165\n",
            "progress: epoch: 965 loss 0.007720190876454683\n",
            "progress: epoch: 966 loss 0.007658939375946133\n",
            "progress: epoch: 967 loss 0.007598188691067475\n",
            "progress: epoch: 968 loss 0.007537934605471481\n",
            "progress: epoch: 969 loss 0.007478172939272862\n",
            "progress: epoch: 970 loss 0.007418899548724926\n",
            "progress: epoch: 971 loss 0.007360110325900376\n",
            "progress: epoch: 972 loss 0.007301801198373888\n",
            "progress: epoch: 973 loss 0.0072439681289082096\n",
            "progress: epoch: 974 loss 0.007186607115142901\n",
            "progress: epoch: 975 loss 0.007129714189285935\n",
            "progress: epoch: 976 loss 0.007073285417807905\n",
            "progress: epoch: 977 loss 0.007017316901139318\n",
            "progress: epoch: 978 loss 0.006961804773370058\n",
            "progress: epoch: 979 loss 0.006906745201952323\n",
            "progress: epoch: 980 loss 0.006852134387405791\n",
            "progress: epoch: 981 loss 0.006797968563025045\n",
            "progress: epoch: 982 loss 0.006744243994590611\n",
            "progress: epoch: 983 loss 0.006690956980081779\n",
            "progress: epoch: 984 loss 0.006638103849392089\n",
            "progress: epoch: 985 loss 0.006585680964048402\n",
            "progress: epoch: 986 loss 0.0065336847169305875\n",
            "progress: epoch: 987 loss 0.006482111531995705\n",
            "progress: epoch: 988 loss 0.006430957864003189\n",
            "progress: epoch: 989 loss 0.006380220198243683\n",
            "progress: epoch: 990 loss 0.006329895050269294\n",
            "progress: epoch: 991 loss 0.0062799789656268114\n",
            "progress: epoch: 992 loss 0.006230468519593461\n",
            "progress: epoch: 993 loss 0.006181360316914465\n",
            "progress: epoch: 994 loss 0.00613265099154374\n",
            "progress: epoch: 995 loss 0.006084337206386026\n",
            "progress: epoch: 996 loss 0.006036415653042148\n",
            "progress: epoch: 997 loss 0.005988883051556195\n",
            "progress: epoch: 998 loss 0.005941736150164922\n",
            "progress: epoch: 999 loss 0.005894971725049225\n",
            "estimation of the parameters: [[ 2.01242982 -2.95372603]] [[0.97245687]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now create Linear Regression and Gradient Descent using pytorch\n",
        "dtype = torch.cuda.FloatTensor"
      ],
      "metadata": {
        "id": "GKilAHHm9qT-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t = torch.from_numpy(x).type(dtype)\n",
        "y_t = torch.from_numpy(y).type(dtype)"
      ],
      "metadata": {
        "id": "DsvffYwfs5og"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = random(2).reshape(1, -1)\n",
        "b_init = random(1).reshape(-1, 1)\n",
        "print(f\"intialized weights: {W}, {b_init}\")\n",
        "W_t = torch.from_numpy(W).type(dtype)\n",
        "b_t = torch.from_numpy(b_init).type(dtype)\n",
        "print(f\"intialized weights as Tensors: {W_t}, {b_t}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu9eyWX2tOTR",
        "outputId": "4ec4855a-ef0c-4ce0-cba5-9ca3ee714c98"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intialized weights: [[0.42220785 0.83675717]], [[0.66164985]]\n",
            "intialized weights as Tensors: tensor([[0.4222, 0.8368]], device='cuda:0'), tensor([[0.6616]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(W, b, X):\n",
        "  return torch.mm(X, W.t())+b\n",
        "y_hat = forward(W_t, b_t, x_t)"
      ],
      "metadata": {
        "id": "Hvrpgh96uIQs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_hat, y):\n",
        "  return (y_hat - y).pow(2).sum()\n",
        "l = loss(y_hat, y_t)"
      ],
      "metadata": {
        "id": "36W4-MtCufy8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(y_hat, y, X):\n",
        "  dW = torch.mm((y_hat - y).t(), X)\n",
        "  db = (y_hat - y).sum()\n",
        "  return 2*dW, 2*db"
      ],
      "metadata": {
        "id": "0kPm5kmlvC6G"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more precise than numpy\n",
        "lr = 1e-2\n",
        "for epoch in range(1000):\n",
        "  y_hat = forward(W_t, b_t, x_t)\n",
        "  l = loss(y_hat, y_t)\n",
        "  if l <= 1e-5:\n",
        "    print(\"Converged!!!\")\n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",l)\n",
        "    break\n",
        "  else:\n",
        "    dw_t, db_t = grad(y_hat, y_t, x_t)\n",
        "    W_t = W_t - lr * dw_t\n",
        "    b_t = b_t - lr * db_t\n",
        "  print(\"progress:\", \"epoch:\", epoch, \"loss\",l)\n",
        "# After training\n",
        "print(\"estimation of the parameters:\", W_t, b_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxHV4G11vxKd",
        "outputId": "92629665-a680-48ab-dcf0-b93a40a88477"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss tensor(55.8100, device='cuda:0')\n",
            "progress: epoch: 1 loss tensor(36.7293, device='cuda:0')\n",
            "progress: epoch: 2 loss tensor(33.0020, device='cuda:0')\n",
            "progress: epoch: 3 loss tensor(29.9360, device='cuda:0')\n",
            "progress: epoch: 4 loss tensor(27.1624, device='cuda:0')\n",
            "progress: epoch: 5 loss tensor(24.6469, device='cuda:0')\n",
            "progress: epoch: 6 loss tensor(22.3651, device='cuda:0')\n",
            "progress: epoch: 7 loss tensor(20.2953, device='cuda:0')\n",
            "progress: epoch: 8 loss tensor(18.4178, device='cuda:0')\n",
            "progress: epoch: 9 loss tensor(16.7147, device='cuda:0')\n",
            "progress: epoch: 10 loss tensor(15.1697, device='cuda:0')\n",
            "progress: epoch: 11 loss tensor(13.7681, device='cuda:0')\n",
            "progress: epoch: 12 loss tensor(12.4965, device='cuda:0')\n",
            "progress: epoch: 13 loss tensor(11.3429, device='cuda:0')\n",
            "progress: epoch: 14 loss tensor(10.2962, device='cuda:0')\n",
            "progress: epoch: 15 loss tensor(9.3466, device='cuda:0')\n",
            "progress: epoch: 16 loss tensor(8.4850, device='cuda:0')\n",
            "progress: epoch: 17 loss tensor(7.7032, device='cuda:0')\n",
            "progress: epoch: 18 loss tensor(6.9937, device='cuda:0')\n",
            "progress: epoch: 19 loss tensor(6.3500, device='cuda:0')\n",
            "progress: epoch: 20 loss tensor(5.7658, device='cuda:0')\n",
            "progress: epoch: 21 loss tensor(5.2356, device='cuda:0')\n",
            "progress: epoch: 22 loss tensor(4.7545, device='cuda:0')\n",
            "progress: epoch: 23 loss tensor(4.3178, device='cuda:0')\n",
            "progress: epoch: 24 loss tensor(3.9215, device='cuda:0')\n",
            "progress: epoch: 25 loss tensor(3.5617, device='cuda:0')\n",
            "progress: epoch: 26 loss tensor(3.2352, device='cuda:0')\n",
            "progress: epoch: 27 loss tensor(2.9388, device='cuda:0')\n",
            "progress: epoch: 28 loss tensor(2.6697, device='cuda:0')\n",
            "progress: epoch: 29 loss tensor(2.4254, device='cuda:0')\n",
            "progress: epoch: 30 loss tensor(2.2036, device='cuda:0')\n",
            "progress: epoch: 31 loss tensor(2.0023, device='cuda:0')\n",
            "progress: epoch: 32 loss tensor(1.8195, device='cuda:0')\n",
            "progress: epoch: 33 loss tensor(1.6535, device='cuda:0')\n",
            "progress: epoch: 34 loss tensor(1.5027, device='cuda:0')\n",
            "progress: epoch: 35 loss tensor(1.3658, device='cuda:0')\n",
            "progress: epoch: 36 loss tensor(1.2415, device='cuda:0')\n",
            "progress: epoch: 37 loss tensor(1.1286, device='cuda:0')\n",
            "progress: epoch: 38 loss tensor(1.0260, device='cuda:0')\n",
            "progress: epoch: 39 loss tensor(0.9328, device='cuda:0')\n",
            "progress: epoch: 40 loss tensor(0.8482, device='cuda:0')\n",
            "progress: epoch: 41 loss tensor(0.7713, device='cuda:0')\n",
            "progress: epoch: 42 loss tensor(0.7015, device='cuda:0')\n",
            "progress: epoch: 43 loss tensor(0.6380, device='cuda:0')\n",
            "progress: epoch: 44 loss tensor(0.5803, device='cuda:0')\n",
            "progress: epoch: 45 loss tensor(0.5279, device='cuda:0')\n",
            "progress: epoch: 46 loss tensor(0.4803, device='cuda:0')\n",
            "progress: epoch: 47 loss tensor(0.4370, device='cuda:0')\n",
            "progress: epoch: 48 loss tensor(0.3977, device='cuda:0')\n",
            "progress: epoch: 49 loss tensor(0.3619, device='cuda:0')\n",
            "progress: epoch: 50 loss tensor(0.3294, device='cuda:0')\n",
            "progress: epoch: 51 loss tensor(0.2999, device='cuda:0')\n",
            "progress: epoch: 52 loss tensor(0.2730, device='cuda:0')\n",
            "progress: epoch: 53 loss tensor(0.2486, device='cuda:0')\n",
            "progress: epoch: 54 loss tensor(0.2263, device='cuda:0')\n",
            "progress: epoch: 55 loss tensor(0.2061, device='cuda:0')\n",
            "progress: epoch: 56 loss tensor(0.1877, device='cuda:0')\n",
            "progress: epoch: 57 loss tensor(0.1710, device='cuda:0')\n",
            "progress: epoch: 58 loss tensor(0.1558, device='cuda:0')\n",
            "progress: epoch: 59 loss tensor(0.1420, device='cuda:0')\n",
            "progress: epoch: 60 loss tensor(0.1294, device='cuda:0')\n",
            "progress: epoch: 61 loss tensor(0.1179, device='cuda:0')\n",
            "progress: epoch: 62 loss tensor(0.1075, device='cuda:0')\n",
            "progress: epoch: 63 loss tensor(0.0980, device='cuda:0')\n",
            "progress: epoch: 64 loss tensor(0.0893, device='cuda:0')\n",
            "progress: epoch: 65 loss tensor(0.0815, device='cuda:0')\n",
            "progress: epoch: 66 loss tensor(0.0743, device='cuda:0')\n",
            "progress: epoch: 67 loss tensor(0.0678, device='cuda:0')\n",
            "progress: epoch: 68 loss tensor(0.0618, device='cuda:0')\n",
            "progress: epoch: 69 loss tensor(0.0564, device='cuda:0')\n",
            "progress: epoch: 70 loss tensor(0.0515, device='cuda:0')\n",
            "progress: epoch: 71 loss tensor(0.0470, device='cuda:0')\n",
            "progress: epoch: 72 loss tensor(0.0429, device='cuda:0')\n",
            "progress: epoch: 73 loss tensor(0.0392, device='cuda:0')\n",
            "progress: epoch: 74 loss tensor(0.0358, device='cuda:0')\n",
            "progress: epoch: 75 loss tensor(0.0327, device='cuda:0')\n",
            "progress: epoch: 76 loss tensor(0.0299, device='cuda:0')\n",
            "progress: epoch: 77 loss tensor(0.0273, device='cuda:0')\n",
            "progress: epoch: 78 loss tensor(0.0249, device='cuda:0')\n",
            "progress: epoch: 79 loss tensor(0.0228, device='cuda:0')\n",
            "progress: epoch: 80 loss tensor(0.0208, device='cuda:0')\n",
            "progress: epoch: 81 loss tensor(0.0190, device='cuda:0')\n",
            "progress: epoch: 82 loss tensor(0.0174, device='cuda:0')\n",
            "progress: epoch: 83 loss tensor(0.0159, device='cuda:0')\n",
            "progress: epoch: 84 loss tensor(0.0146, device='cuda:0')\n",
            "progress: epoch: 85 loss tensor(0.0133, device='cuda:0')\n",
            "progress: epoch: 86 loss tensor(0.0122, device='cuda:0')\n",
            "progress: epoch: 87 loss tensor(0.0112, device='cuda:0')\n",
            "progress: epoch: 88 loss tensor(0.0102, device='cuda:0')\n",
            "progress: epoch: 89 loss tensor(0.0094, device='cuda:0')\n",
            "progress: epoch: 90 loss tensor(0.0086, device='cuda:0')\n",
            "progress: epoch: 91 loss tensor(0.0079, device='cuda:0')\n",
            "progress: epoch: 92 loss tensor(0.0072, device='cuda:0')\n",
            "progress: epoch: 93 loss tensor(0.0066, device='cuda:0')\n",
            "progress: epoch: 94 loss tensor(0.0060, device='cuda:0')\n",
            "progress: epoch: 95 loss tensor(0.0055, device='cuda:0')\n",
            "progress: epoch: 96 loss tensor(0.0051, device='cuda:0')\n",
            "progress: epoch: 97 loss tensor(0.0047, device='cuda:0')\n",
            "progress: epoch: 98 loss tensor(0.0043, device='cuda:0')\n",
            "progress: epoch: 99 loss tensor(0.0039, device='cuda:0')\n",
            "progress: epoch: 100 loss tensor(0.0036, device='cuda:0')\n",
            "progress: epoch: 101 loss tensor(0.0033, device='cuda:0')\n",
            "progress: epoch: 102 loss tensor(0.0030, device='cuda:0')\n",
            "progress: epoch: 103 loss tensor(0.0028, device='cuda:0')\n",
            "progress: epoch: 104 loss tensor(0.0026, device='cuda:0')\n",
            "progress: epoch: 105 loss tensor(0.0024, device='cuda:0')\n",
            "progress: epoch: 106 loss tensor(0.0022, device='cuda:0')\n",
            "progress: epoch: 107 loss tensor(0.0020, device='cuda:0')\n",
            "progress: epoch: 108 loss tensor(0.0018, device='cuda:0')\n",
            "progress: epoch: 109 loss tensor(0.0017, device='cuda:0')\n",
            "progress: epoch: 110 loss tensor(0.0015, device='cuda:0')\n",
            "progress: epoch: 111 loss tensor(0.0014, device='cuda:0')\n",
            "progress: epoch: 112 loss tensor(0.0013, device='cuda:0')\n",
            "progress: epoch: 113 loss tensor(0.0012, device='cuda:0')\n",
            "progress: epoch: 114 loss tensor(0.0011, device='cuda:0')\n",
            "progress: epoch: 115 loss tensor(0.0010, device='cuda:0')\n",
            "progress: epoch: 116 loss tensor(0.0009, device='cuda:0')\n",
            "progress: epoch: 117 loss tensor(0.0009, device='cuda:0')\n",
            "progress: epoch: 118 loss tensor(0.0008, device='cuda:0')\n",
            "progress: epoch: 119 loss tensor(0.0007, device='cuda:0')\n",
            "progress: epoch: 120 loss tensor(0.0007, device='cuda:0')\n",
            "progress: epoch: 121 loss tensor(0.0006, device='cuda:0')\n",
            "progress: epoch: 122 loss tensor(0.0006, device='cuda:0')\n",
            "progress: epoch: 123 loss tensor(0.0005, device='cuda:0')\n",
            "progress: epoch: 124 loss tensor(0.0005, device='cuda:0')\n",
            "progress: epoch: 125 loss tensor(0.0005, device='cuda:0')\n",
            "progress: epoch: 126 loss tensor(0.0004, device='cuda:0')\n",
            "progress: epoch: 127 loss tensor(0.0004, device='cuda:0')\n",
            "progress: epoch: 128 loss tensor(0.0004, device='cuda:0')\n",
            "progress: epoch: 129 loss tensor(0.0003, device='cuda:0')\n",
            "progress: epoch: 130 loss tensor(0.0003, device='cuda:0')\n",
            "progress: epoch: 131 loss tensor(0.0003, device='cuda:0')\n",
            "progress: epoch: 132 loss tensor(0.0003, device='cuda:0')\n",
            "progress: epoch: 133 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 134 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 135 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 136 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 137 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 138 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 139 loss tensor(0.0002, device='cuda:0')\n",
            "progress: epoch: 140 loss tensor(0.0001, device='cuda:0')\n",
            "progress: epoch: 141 loss tensor(0.0001, device='cuda:0')\n",
            "progress: epoch: 142 loss tensor(0.0001, device='cuda:0')\n",
            "progress: epoch: 143 loss tensor(0.0001, device='cuda:0')\n",
            "progress: epoch: 144 loss tensor(0.0001, device='cuda:0')\n",
            "progress: epoch: 145 loss tensor(9.4581e-05, device='cuda:0')\n",
            "progress: epoch: 146 loss tensor(8.7622e-05, device='cuda:0')\n",
            "progress: epoch: 147 loss tensor(8.1188e-05, device='cuda:0')\n",
            "progress: epoch: 148 loss tensor(7.5235e-05, device='cuda:0')\n",
            "progress: epoch: 149 loss tensor(6.9734e-05, device='cuda:0')\n",
            "progress: epoch: 150 loss tensor(6.4644e-05, device='cuda:0')\n",
            "progress: epoch: 151 loss tensor(5.9932e-05, device='cuda:0')\n",
            "progress: epoch: 152 loss tensor(5.5572e-05, device='cuda:0')\n",
            "progress: epoch: 153 loss tensor(5.1537e-05, device='cuda:0')\n",
            "progress: epoch: 154 loss tensor(4.7799e-05, device='cuda:0')\n",
            "progress: epoch: 155 loss tensor(4.4342e-05, device='cuda:0')\n",
            "progress: epoch: 156 loss tensor(4.1139e-05, device='cuda:0')\n",
            "progress: epoch: 157 loss tensor(3.8174e-05, device='cuda:0')\n",
            "progress: epoch: 158 loss tensor(3.5429e-05, device='cuda:0')\n",
            "progress: epoch: 159 loss tensor(3.2882e-05, device='cuda:0')\n",
            "progress: epoch: 160 loss tensor(3.0525e-05, device='cuda:0')\n",
            "progress: epoch: 161 loss tensor(2.8341e-05, device='cuda:0')\n",
            "progress: epoch: 162 loss tensor(2.6315e-05, device='cuda:0')\n",
            "progress: epoch: 163 loss tensor(2.4437e-05, device='cuda:0')\n",
            "progress: epoch: 164 loss tensor(2.2698e-05, device='cuda:0')\n",
            "progress: epoch: 165 loss tensor(2.1084e-05, device='cuda:0')\n",
            "progress: epoch: 166 loss tensor(1.9588e-05, device='cuda:0')\n",
            "progress: epoch: 167 loss tensor(1.8198e-05, device='cuda:0')\n",
            "progress: epoch: 168 loss tensor(1.6910e-05, device='cuda:0')\n",
            "progress: epoch: 169 loss tensor(1.5714e-05, device='cuda:0')\n",
            "progress: epoch: 170 loss tensor(1.4605e-05, device='cuda:0')\n",
            "progress: epoch: 171 loss tensor(1.3575e-05, device='cuda:0')\n",
            "progress: epoch: 172 loss tensor(1.2618e-05, device='cuda:0')\n",
            "progress: epoch: 173 loss tensor(1.1731e-05, device='cuda:0')\n",
            "progress: epoch: 174 loss tensor(1.0907e-05, device='cuda:0')\n",
            "progress: epoch: 175 loss tensor(1.0142e-05, device='cuda:0')\n",
            "Converged!!!\n",
            "progress: epoch: 176 loss tensor(9.4306e-06, device='cuda:0')\n",
            "estimation of the parameters: tensor([[ 2.0011, -2.9984]], device='cuda:0') tensor([[0.9987]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implement linear regression and gradient descent using Autograd...\n",
        "W = random(2).reshape(1, -1)\n",
        "b_init = random(1).reshape(-1, 1)\n",
        "print(f\"intialized weights: {W}, {b_init}\")\n",
        "W_t = torch.from_numpy(W).type(dtype)\n",
        "b_t = torch.from_numpy(b_init).type(dtype)\n",
        "W_t.requires_grad_()\n",
        "b_t.requires_grad_()\n",
        "print(f\"intialized weights as Tensors: {W_t}, {b_t}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkwvHZ3TwnZB",
        "outputId": "99dc5e31-9080-455b-e478-421438b95d7a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intialized weights: [[0.96124238 0.33311662]], [[0.28585037]]\n",
            "intialized weights as Tensors: tensor([[0.9612, 0.3331]], device='cuda:0', requires_grad=True), tensor([[0.2859]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(300):\n",
        "  y_hat = torch.mm(x_t, W_t.t()) + b_t\n",
        "  loss = (y_hat - y_t).pow(2).sum()\n",
        "\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    W_t -= lr * W_t.grad\n",
        "    b_t -= lr * b_t.grad\n",
        "  # manually set the gradient to zero.\n",
        "  W_t.grad.zero_()\n",
        "  b_t.grad.zero_()\n",
        "  print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.data.item())\n",
        "# After training\n",
        "print(\"estimation of the parameters:\", W_t.data, b_t.data.t())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_DDrHVEykMq",
        "outputId": "7ddf052c-a6ea-407c-e6de-79d514f29514"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 33.11903762817383\n",
            "progress: epoch: 1 loss 26.545928955078125\n",
            "progress: epoch: 2 loss 24.045360565185547\n",
            "progress: epoch: 3 loss 21.854766845703125\n",
            "progress: epoch: 4 loss 19.867050170898438\n",
            "progress: epoch: 5 loss 18.061668395996094\n",
            "progress: epoch: 6 loss 16.421749114990234\n",
            "progress: epoch: 7 loss 14.932035446166992\n",
            "progress: epoch: 8 loss 13.5786771774292\n",
            "progress: epoch: 9 loss 12.349108695983887\n",
            "progress: epoch: 10 loss 11.231931686401367\n",
            "progress: epoch: 11 loss 10.216796875\n",
            "progress: epoch: 12 loss 9.294320106506348\n",
            "progress: epoch: 13 loss 8.455978393554688\n",
            "progress: epoch: 14 loss 7.694040298461914\n",
            "progress: epoch: 15 loss 7.001488208770752\n",
            "progress: epoch: 16 loss 6.371952056884766\n",
            "progress: epoch: 17 loss 5.799652099609375\n",
            "progress: epoch: 18 loss 5.2793402671813965\n",
            "progress: epoch: 19 loss 4.806252479553223\n",
            "progress: epoch: 20 loss 4.376064777374268\n",
            "progress: epoch: 21 loss 3.984851837158203\n",
            "progress: epoch: 22 loss 3.629049777984619\n",
            "progress: epoch: 23 loss 3.3054234981536865\n",
            "progress: epoch: 24 loss 3.011033535003662\n",
            "progress: epoch: 25 loss 2.743213176727295\n",
            "progress: epoch: 26 loss 2.4995386600494385\n",
            "progress: epoch: 27 loss 2.2778115272521973\n",
            "progress: epoch: 28 loss 2.076033115386963\n",
            "progress: epoch: 29 loss 1.8923895359039307\n",
            "progress: epoch: 30 loss 1.7252323627471924\n",
            "progress: epoch: 31 loss 1.5730634927749634\n",
            "progress: epoch: 32 loss 1.4345238208770752\n",
            "progress: epoch: 33 loss 1.3083786964416504\n",
            "progress: epoch: 34 loss 1.1935042142868042\n",
            "progress: epoch: 35 loss 1.0888817310333252\n",
            "progress: epoch: 36 loss 0.9935837388038635\n",
            "progress: epoch: 37 loss 0.9067687392234802\n",
            "progress: epoch: 38 loss 0.8276710510253906\n",
            "progress: epoch: 39 loss 0.7555956244468689\n",
            "progress: epoch: 40 loss 0.6899099946022034\n",
            "progress: epoch: 41 loss 0.6300398111343384\n",
            "progress: epoch: 42 loss 0.5754619836807251\n",
            "progress: epoch: 43 loss 0.5257019996643066\n",
            "progress: epoch: 44 loss 0.4803285002708435\n",
            "progress: epoch: 45 loss 0.43894830346107483\n",
            "progress: epoch: 46 loss 0.40120482444763184\n",
            "progress: epoch: 47 loss 0.3667725920677185\n",
            "progress: epoch: 48 loss 0.3353567123413086\n",
            "progress: epoch: 49 loss 0.3066885471343994\n",
            "progress: epoch: 50 loss 0.280523419380188\n",
            "progress: epoch: 51 loss 0.2566392123699188\n",
            "progress: epoch: 52 loss 0.23483332991600037\n",
            "progress: epoch: 53 loss 0.21492156386375427\n",
            "progress: epoch: 54 loss 0.1967363804578781\n",
            "progress: epoch: 55 loss 0.18012528121471405\n",
            "progress: epoch: 56 loss 0.16494953632354736\n",
            "progress: epoch: 57 loss 0.15108244121074677\n",
            "progress: epoch: 58 loss 0.13840927183628082\n",
            "progress: epoch: 59 loss 0.12682469189167023\n",
            "progress: epoch: 60 loss 0.11623365432024002\n",
            "progress: epoch: 61 loss 0.1065489649772644\n",
            "progress: epoch: 62 loss 0.09769148379564285\n",
            "progress: epoch: 63 loss 0.08958910405635834\n",
            "progress: epoch: 64 loss 0.08217602968215942\n",
            "progress: epoch: 65 loss 0.07539220154285431\n",
            "progress: epoch: 66 loss 0.06918317079544067\n",
            "progress: epoch: 67 loss 0.0634988322854042\n",
            "progress: epoch: 68 loss 0.05829412862658501\n",
            "progress: epoch: 69 loss 0.05352741479873657\n",
            "progress: epoch: 70 loss 0.049161139875650406\n",
            "progress: epoch: 71 loss 0.045160770416259766\n",
            "progress: epoch: 72 loss 0.04149492457509041\n",
            "progress: epoch: 73 loss 0.03813481703400612\n",
            "progress: epoch: 74 loss 0.035054489970207214\n",
            "progress: epoch: 75 loss 0.03222988173365593\n",
            "progress: epoch: 76 loss 0.029639385640621185\n",
            "progress: epoch: 77 loss 0.027263062074780464\n",
            "progress: epoch: 78 loss 0.025082658976316452\n",
            "progress: epoch: 79 loss 0.0230817012488842\n",
            "progress: epoch: 80 loss 0.02124486118555069\n",
            "progress: epoch: 81 loss 0.019558507949113846\n",
            "progress: epoch: 82 loss 0.018009915947914124\n",
            "progress: epoch: 83 loss 0.01658748835325241\n",
            "progress: epoch: 84 loss 0.015280717983841896\n",
            "progress: epoch: 85 loss 0.01407987903803587\n",
            "progress: epoch: 86 loss 0.012976201251149178\n",
            "progress: epoch: 87 loss 0.011961566284298897\n",
            "progress: epoch: 88 loss 0.011028597131371498\n",
            "progress: epoch: 89 loss 0.010170536115765572\n",
            "progress: epoch: 90 loss 0.009381206706166267\n",
            "progress: epoch: 91 loss 0.008654965087771416\n",
            "progress: epoch: 92 loss 0.00798656977713108\n",
            "progress: epoch: 93 loss 0.0073712971061468124\n",
            "progress: epoch: 94 loss 0.006804864853620529\n",
            "progress: epoch: 95 loss 0.006283162161707878\n",
            "progress: epoch: 96 loss 0.005802653729915619\n",
            "progress: epoch: 97 loss 0.005359933711588383\n",
            "progress: epoch: 98 loss 0.004952008370310068\n",
            "progress: epoch: 99 loss 0.004576012026518583\n",
            "progress: epoch: 100 loss 0.004229354672133923\n",
            "progress: epoch: 101 loss 0.003909734543412924\n",
            "progress: epoch: 102 loss 0.0036149427760392427\n",
            "progress: epoch: 103 loss 0.0033430312760174274\n",
            "progress: epoch: 104 loss 0.003092128550633788\n",
            "progress: epoch: 105 loss 0.0028605679981410503\n",
            "progress: epoch: 106 loss 0.0026468734722584486\n",
            "progress: epoch: 107 loss 0.002449529245495796\n",
            "progress: epoch: 108 loss 0.002267322037369013\n",
            "progress: epoch: 109 loss 0.0020990436896681786\n",
            "progress: epoch: 110 loss 0.0019435862777754664\n",
            "progress: epoch: 111 loss 0.001799941179342568\n",
            "progress: epoch: 112 loss 0.0016672033816576004\n",
            "progress: epoch: 113 loss 0.001544483588077128\n",
            "progress: epoch: 114 loss 0.0014310672413557768\n",
            "progress: epoch: 115 loss 0.0013261593412607908\n",
            "progress: epoch: 116 loss 0.001229150453582406\n",
            "progress: epoch: 117 loss 0.0011394117027521133\n",
            "progress: epoch: 118 loss 0.001056385226547718\n",
            "progress: epoch: 119 loss 0.0009795563528314233\n",
            "progress: epoch: 120 loss 0.0009084427729249001\n",
            "progress: epoch: 121 loss 0.000842629699036479\n",
            "progress: epoch: 122 loss 0.0007816891884431243\n",
            "progress: epoch: 123 loss 0.0007252662908285856\n",
            "progress: epoch: 124 loss 0.0006730096065439284\n",
            "progress: epoch: 125 loss 0.000624604057520628\n",
            "progress: epoch: 126 loss 0.0005797434132546186\n",
            "progress: epoch: 127 loss 0.0005381843075156212\n",
            "progress: epoch: 128 loss 0.0004996662028133869\n",
            "progress: epoch: 129 loss 0.00046396933612413704\n",
            "progress: epoch: 130 loss 0.0004308739153202623\n",
            "progress: epoch: 131 loss 0.0004001805791631341\n",
            "progress: epoch: 132 loss 0.0003717238432727754\n",
            "progress: epoch: 133 loss 0.00034532565041445196\n",
            "progress: epoch: 134 loss 0.0003208409179933369\n",
            "progress: epoch: 135 loss 0.0002981325378641486\n",
            "progress: epoch: 136 loss 0.000277055602055043\n",
            "progress: epoch: 137 loss 0.00025749957421794534\n",
            "progress: epoch: 138 loss 0.0002393457107245922\n",
            "progress: epoch: 139 loss 0.00022249901667237282\n",
            "progress: epoch: 140 loss 0.00020685693016275764\n",
            "progress: epoch: 141 loss 0.0001923287782119587\n",
            "progress: epoch: 142 loss 0.00017884676344692707\n",
            "progress: epoch: 143 loss 0.0001663214061409235\n",
            "progress: epoch: 144 loss 0.00015468429774045944\n",
            "progress: epoch: 145 loss 0.0001438765466446057\n",
            "progress: epoch: 146 loss 0.00013383905752561986\n",
            "progress: epoch: 147 loss 0.00012450931535568088\n",
            "progress: epoch: 148 loss 0.00011584062303882092\n",
            "progress: epoch: 149 loss 0.00010778206342365593\n",
            "progress: epoch: 150 loss 0.0001002952631097287\n",
            "progress: epoch: 151 loss 9.333802154287696e-05\n",
            "progress: epoch: 152 loss 8.68644638103433e-05\n",
            "progress: epoch: 153 loss 8.08482727734372e-05\n",
            "progress: epoch: 154 loss 7.525379623984918e-05\n",
            "progress: epoch: 155 loss 7.00522432452999e-05\n",
            "progress: epoch: 156 loss 6.52114613330923e-05\n",
            "progress: epoch: 157 loss 6.071317329769954e-05\n",
            "progress: epoch: 158 loss 5.652670006384142e-05\n",
            "progress: epoch: 159 loss 5.263482307782397e-05\n",
            "progress: epoch: 160 loss 4.901239663013257e-05\n",
            "progress: epoch: 161 loss 4.564540722640231e-05\n",
            "progress: epoch: 162 loss 4.251072823535651e-05\n",
            "progress: epoch: 163 loss 3.959466630476527e-05\n",
            "progress: epoch: 164 loss 3.6877714592264965e-05\n",
            "progress: epoch: 165 loss 3.4348755434621125e-05\n",
            "progress: epoch: 166 loss 3.199882849003188e-05\n",
            "progress: epoch: 167 loss 2.9807339160470292e-05\n",
            "progress: epoch: 168 loss 2.776830478978809e-05\n",
            "progress: epoch: 169 loss 2.5867857402772643e-05\n",
            "progress: epoch: 170 loss 2.4100711016217247e-05\n",
            "progress: epoch: 171 loss 2.245606083306484e-05\n",
            "progress: epoch: 172 loss 2.0922916519339196e-05\n",
            "progress: epoch: 173 loss 1.9496736058499664e-05\n",
            "progress: epoch: 174 loss 1.8168502720072865e-05\n",
            "progress: epoch: 175 loss 1.693114791123662e-05\n",
            "progress: epoch: 176 loss 1.5780529793119058e-05\n",
            "progress: epoch: 177 loss 1.4706454749102704e-05\n",
            "progress: epoch: 178 loss 1.3707645848626271e-05\n",
            "progress: epoch: 179 loss 1.2777995834767353e-05\n",
            "progress: epoch: 180 loss 1.190992770716548e-05\n",
            "progress: epoch: 181 loss 1.1101526069978718e-05\n",
            "progress: epoch: 182 loss 1.0349287549615838e-05\n",
            "progress: epoch: 183 loss 9.647895240050275e-06\n",
            "progress: epoch: 184 loss 8.995067219075281e-06\n",
            "progress: epoch: 185 loss 8.385087312490214e-06\n",
            "progress: epoch: 186 loss 7.817909136065282e-06\n",
            "progress: epoch: 187 loss 7.288254437298747e-06\n",
            "progress: epoch: 188 loss 6.79546747051063e-06\n",
            "progress: epoch: 189 loss 6.335404577839654e-06\n",
            "progress: epoch: 190 loss 5.906952083023498e-06\n",
            "progress: epoch: 191 loss 5.507847163244151e-06\n",
            "progress: epoch: 192 loss 5.1353526941966265e-06\n",
            "progress: epoch: 193 loss 4.789162630913779e-06\n",
            "progress: epoch: 194 loss 4.466159680305282e-06\n",
            "progress: epoch: 195 loss 4.163895937381312e-06\n",
            "progress: epoch: 196 loss 3.883144017891027e-06\n",
            "progress: epoch: 197 loss 3.6210944927006494e-06\n",
            "progress: epoch: 198 loss 3.3768910725484602e-06\n",
            "progress: epoch: 199 loss 3.149173608107958e-06\n",
            "progress: epoch: 200 loss 2.937700628535822e-06\n",
            "progress: epoch: 201 loss 2.739453520916868e-06\n",
            "progress: epoch: 202 loss 2.5546705728629604e-06\n",
            "progress: epoch: 203 loss 2.3829363726690644e-06\n",
            "progress: epoch: 204 loss 2.2230749436857877e-06\n",
            "progress: epoch: 205 loss 2.073624045806355e-06\n",
            "progress: epoch: 206 loss 1.9340495782671496e-06\n",
            "progress: epoch: 207 loss 1.8040886970993597e-06\n",
            "progress: epoch: 208 loss 1.6822048110043397e-06\n",
            "progress: epoch: 209 loss 1.5698078641435131e-06\n",
            "progress: epoch: 210 loss 1.4642798760178266e-06\n",
            "progress: epoch: 211 loss 1.3663182016898645e-06\n",
            "progress: epoch: 212 loss 1.2746229458571179e-06\n",
            "progress: epoch: 213 loss 1.189077238450409e-06\n",
            "progress: epoch: 214 loss 1.1091615306213498e-06\n",
            "progress: epoch: 215 loss 1.0348143177907332e-06\n",
            "progress: epoch: 216 loss 9.653545021137688e-07\n",
            "progress: epoch: 217 loss 9.006716936710291e-07\n",
            "progress: epoch: 218 loss 8.404593359045975e-07\n",
            "progress: epoch: 219 loss 7.841156275389949e-07\n",
            "progress: epoch: 220 loss 7.313275318665546e-07\n",
            "progress: epoch: 221 loss 6.828319101259694e-07\n",
            "progress: epoch: 222 loss 6.370192977556144e-07\n",
            "progress: epoch: 223 loss 5.942219445387309e-07\n",
            "progress: epoch: 224 loss 5.545923613681225e-07\n",
            "progress: epoch: 225 loss 5.170246595298522e-07\n",
            "progress: epoch: 226 loss 4.825636779060005e-07\n",
            "progress: epoch: 227 loss 4.500597015066887e-07\n",
            "progress: epoch: 228 loss 4.201363310585293e-07\n",
            "progress: epoch: 229 loss 3.919626010429056e-07\n",
            "progress: epoch: 230 loss 3.6571995565282123e-07\n",
            "progress: epoch: 231 loss 3.4139401350330445e-07\n",
            "progress: epoch: 232 loss 3.184088939178764e-07\n",
            "progress: epoch: 233 loss 2.9725117656198563e-07\n",
            "progress: epoch: 234 loss 2.773964524749317e-07\n",
            "progress: epoch: 235 loss 2.589259793239762e-07\n",
            "progress: epoch: 236 loss 2.415685571577342e-07\n",
            "progress: epoch: 237 loss 2.252885451525799e-07\n",
            "progress: epoch: 238 loss 2.1025448404543567e-07\n",
            "progress: epoch: 239 loss 1.9617478130840027e-07\n",
            "progress: epoch: 240 loss 1.8301913939922088e-07\n",
            "progress: epoch: 241 loss 1.7080961356441549e-07\n",
            "progress: epoch: 242 loss 1.5948182863212423e-07\n",
            "progress: epoch: 243 loss 1.486741894041188e-07\n",
            "progress: epoch: 244 loss 1.3880035965030402e-07\n",
            "progress: epoch: 245 loss 1.294861675660286e-07\n",
            "progress: epoch: 246 loss 1.20860278229884e-07\n",
            "progress: epoch: 247 loss 1.1277190026248718e-07\n",
            "progress: epoch: 248 loss 1.0525765503643925e-07\n",
            "progress: epoch: 249 loss 9.816631063586101e-08\n",
            "progress: epoch: 250 loss 9.164455150312278e-08\n",
            "progress: epoch: 251 loss 8.558035347050463e-08\n",
            "progress: epoch: 252 loss 7.979299709859333e-08\n",
            "progress: epoch: 253 loss 7.451502881394845e-08\n",
            "progress: epoch: 254 loss 6.950796205273946e-08\n",
            "progress: epoch: 255 loss 6.488326675935241e-08\n",
            "progress: epoch: 256 loss 6.056626489225891e-08\n",
            "progress: epoch: 257 loss 5.642722911147757e-08\n",
            "progress: epoch: 258 loss 5.275272130234043e-08\n",
            "progress: epoch: 259 loss 4.921425400539192e-08\n",
            "progress: epoch: 260 loss 4.59660114415783e-08\n",
            "progress: epoch: 261 loss 4.2886021844878996e-08\n",
            "progress: epoch: 262 loss 4.0108336207822504e-08\n",
            "progress: epoch: 263 loss 3.7336388203357274e-08\n",
            "progress: epoch: 264 loss 3.4922969405215554e-08\n",
            "progress: epoch: 265 loss 3.262432812789484e-08\n",
            "progress: epoch: 266 loss 3.037317242160498e-08\n",
            "progress: epoch: 267 loss 2.835453116745157e-08\n",
            "progress: epoch: 268 loss 2.6423936816399873e-08\n",
            "progress: epoch: 269 loss 2.4745883564492033e-08\n",
            "progress: epoch: 270 loss 2.310240532210628e-08\n",
            "progress: epoch: 271 loss 2.1576953557200795e-08\n",
            "progress: epoch: 272 loss 2.0088135599394263e-08\n",
            "progress: epoch: 273 loss 1.8754153785494054e-08\n",
            "progress: epoch: 274 loss 1.748164102366445e-08\n",
            "progress: epoch: 275 loss 1.633156188063367e-08\n",
            "progress: epoch: 276 loss 1.5289307597754487e-08\n",
            "progress: epoch: 277 loss 1.4242176327172729e-08\n",
            "progress: epoch: 278 loss 1.3296777012783423e-08\n",
            "progress: epoch: 279 loss 1.240545088876388e-08\n",
            "progress: epoch: 280 loss 1.1603992433606436e-08\n",
            "progress: epoch: 281 loss 1.0828861363165743e-08\n",
            "progress: epoch: 282 loss 1.0109051373774491e-08\n",
            "progress: epoch: 283 loss 9.429625080770165e-09\n",
            "progress: epoch: 284 loss 8.801194439911342e-09\n",
            "progress: epoch: 285 loss 8.171554988223306e-09\n",
            "progress: epoch: 286 loss 7.643310873106657e-09\n",
            "progress: epoch: 287 loss 7.166641946554364e-09\n",
            "progress: epoch: 288 loss 6.680435760131331e-09\n",
            "progress: epoch: 289 loss 6.239743832736622e-09\n",
            "progress: epoch: 290 loss 5.845074202426304e-09\n",
            "progress: epoch: 291 loss 5.450663032036118e-09\n",
            "progress: epoch: 292 loss 5.097594790726134e-09\n",
            "progress: epoch: 293 loss 4.765216665703065e-09\n",
            "progress: epoch: 294 loss 4.447433088472508e-09\n",
            "progress: epoch: 295 loss 4.138029918721031e-09\n",
            "progress: epoch: 296 loss 3.856376995514665e-09\n",
            "progress: epoch: 297 loss 3.5980227686138733e-09\n",
            "progress: epoch: 298 loss 3.3947284983071313e-09\n",
            "progress: epoch: 299 loss 3.1679547873864067e-09\n",
            "estimation of the parameters: tensor([[ 2.0000, -3.0000]], device='cuda:0') tensor([[1.0000]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfqvSw52zt4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}