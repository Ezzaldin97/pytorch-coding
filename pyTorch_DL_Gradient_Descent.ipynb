{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxE82T6mM74QiuEXPEMFgE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ezzaldin97/pytorch-coding/blob/main/pyTorch_DL_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6RLLWmQCjh8N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create Gradient Descent using numpy manually...\n",
        "# y = 2 * X\n",
        "X = np.array([1, 2, 3, 4], dtype = np.float32)\n",
        "y = np.array([2, 4, 6, 8])\n",
        "weights = 0.03\n",
        "def forward(x, weights):\n",
        "  return weights * x\n",
        "def loss_func(y, y_hat):\n",
        "  return ((y_hat - y) ** 2).mean()\n",
        "# mse = 1/n * (w*x - y)**2\n",
        "# grad(mse) = dmse/dw = 2 * x * (w * x - y) / n\n",
        "def grad(x, y, y_hat):\n",
        "  return np.dot(2 * x, y_hat - y).mean()\n",
        "lr = 0.01\n",
        "for _ in range(10):\n",
        "  y_hat = forward(X, weights)\n",
        "  print(f\"y_hat:{y_hat}\")\n",
        "  if loss_func(y, y_hat) > 0.001:\n",
        "    print(loss_func(y, y_hat))\n",
        "    # do gradient descent\n",
        "    dweights = grad(X, y, y_hat)\n",
        "    weights = weights - lr * dweights\n",
        "  else:\n",
        "    print(\"Optimal minima...\")\n",
        "    print(f\"y_hat:{y_hat}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdoSmMOAkK8g",
        "outputId": "1c71f3a4-b029-445c-9df6-34ccbb660e3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat:[0.03 0.06 0.09 0.12]\n",
            "29.106750025318934\n",
            "y_hat:[1.212     2.424     3.6360002 4.848    ]\n",
            "4.657079712553031\n",
            "y_hat:[1.6848 3.3696 5.0544 6.7392]\n",
            "0.7451327201911973\n",
            "y_hat:[1.87392 3.74784 5.62176 7.49568]\n",
            "0.11922131639209965\n",
            "y_hat:[1.949568  3.899136  5.8487043 7.798272 ]\n",
            "0.019075356515102726\n",
            "y_hat:[1.9798272 3.9596543 5.9394817 7.9193087]\n",
            "0.003052067142519377\n",
            "y_hat:[1.9919308 3.9838617 5.9757924 7.9677234]\n",
            "Optimal minima...\n",
            "y_hat:[1.9919308 3.9838617 5.9757924 7.9677234]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply gradient descent using pytorch...\n",
        "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "y = torch.tensor([2, 4, 6, 8], dtype = torch.float32)\n",
        "weights = torch.tensor(0.03, dtype = torch.float32, requires_grad=True)"
      ],
      "metadata": {
        "id": "fVoEPpSdnPxJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(100):\n",
        "  y_hat = forward(X, weights)\n",
        "  print(f\"y_hat:{y_hat}\")\n",
        "  if loss_func(y, y_hat) > torch.tensor(0.003):\n",
        "    l = loss_func(y, y_hat)\n",
        "    print(l)\n",
        "  # backpropagation: dloss/dw : dw\n",
        "    l.backward()\n",
        "    with torch.no_grad():\n",
        "      weights -= lr * weights.grad\n",
        "    weights.grad.zero_()\n",
        "  else:\n",
        "    print(\"Optimal Minima\")\n",
        "    print(f\"yhat: {y_hat}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz-nI0uto6mI",
        "outputId": "166543f9-2f43-4eb9-c273-0dc4a9391dfa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat:tensor([0.0300, 0.0600, 0.0900, 0.1200], grad_fn=<MulBackward0>)\n",
            "tensor(29.1068, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([0.3255, 0.6510, 0.9765, 1.3020], grad_fn=<MulBackward0>)\n",
            "tensor(21.0296, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([0.5767, 1.1533, 1.7300, 2.3067], grad_fn=<MulBackward0>)\n",
            "tensor(15.1939, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([0.7902, 1.5803, 2.3705, 3.1607], grad_fn=<MulBackward0>)\n",
            "tensor(10.9776, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([0.9716, 1.9433, 2.9149, 3.8866], grad_fn=<MulBackward0>)\n",
            "tensor(7.9313, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.1259, 2.2518, 3.3777, 4.5036], grad_fn=<MulBackward0>)\n",
            "tensor(5.7304, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.2570, 2.5140, 3.7710, 5.0281], grad_fn=<MulBackward0>)\n",
            "tensor(4.1402, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.3685, 2.7369, 4.1054, 5.4739], grad_fn=<MulBackward0>)\n",
            "tensor(2.9913, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.4632, 2.9264, 4.3896, 5.8528], grad_fn=<MulBackward0>)\n",
            "tensor(2.1612, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.5437, 3.0874, 4.6311, 6.1749], grad_fn=<MulBackward0>)\n",
            "tensor(1.5615, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.6122, 3.2243, 4.8365, 6.4486], grad_fn=<MulBackward0>)\n",
            "tensor(1.1282, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.6703, 3.3407, 5.0110, 6.6813], grad_fn=<MulBackward0>)\n",
            "tensor(0.8151, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.7198, 3.4396, 5.1594, 6.8791], grad_fn=<MulBackward0>)\n",
            "tensor(0.5889, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.7618, 3.5236, 5.2854, 7.0473], grad_fn=<MulBackward0>)\n",
            "tensor(0.4255, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.7975, 3.5951, 5.3926, 7.1902], grad_fn=<MulBackward0>)\n",
            "tensor(0.3074, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.8279, 3.6558, 5.4837, 7.3116], grad_fn=<MulBackward0>)\n",
            "tensor(0.2221, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.8537, 3.7075, 5.5612, 7.4149], grad_fn=<MulBackward0>)\n",
            "tensor(0.1605, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.8757, 3.7513, 5.6270, 7.5027], grad_fn=<MulBackward0>)\n",
            "tensor(0.1159, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.8943, 3.7886, 5.6830, 7.5773], grad_fn=<MulBackward0>)\n",
            "tensor(0.0838, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9102, 3.8203, 5.7305, 7.6407], grad_fn=<MulBackward0>)\n",
            "tensor(0.0605, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9236, 3.8473, 5.7709, 7.6946], grad_fn=<MulBackward0>)\n",
            "tensor(0.0437, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9351, 3.8702, 5.8053, 7.7404], grad_fn=<MulBackward0>)\n",
            "tensor(0.0316, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9448, 3.8897, 5.8345, 7.7793], grad_fn=<MulBackward0>)\n",
            "tensor(0.0228, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9531, 3.9062, 5.8593, 7.8124], grad_fn=<MulBackward0>)\n",
            "tensor(0.0165, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9601, 3.9203, 5.8804, 7.8406], grad_fn=<MulBackward0>)\n",
            "tensor(0.0119, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9661, 3.9322, 5.8984, 7.8645], grad_fn=<MulBackward0>)\n",
            "tensor(0.0086, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9712, 3.9424, 5.9136, 7.8848], grad_fn=<MulBackward0>)\n",
            "tensor(0.0062, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9755, 3.9510, 5.9266, 7.9021], grad_fn=<MulBackward0>)\n",
            "tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9792, 3.9584, 5.9376, 7.9168], grad_fn=<MulBackward0>)\n",
            "tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "y_hat:tensor([1.9823, 3.9646, 5.9469, 7.9293], grad_fn=<MulBackward0>)\n",
            "Optimal Minima\n",
            "yhat: tensor([1.9823, 3.9646, 5.9469, 7.9293], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRwsVO9Cp7dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}